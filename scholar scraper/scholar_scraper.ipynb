{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url= 'https://scholar.google.com/scholar'\n",
    "\n",
    "#set headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \\\n",
    "    AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36' \n",
    "    #english\n",
    "}\n",
    "\n",
    "params = {\n",
    "    #english\n",
    "    'hl': 'en',\n",
    "\n",
    "    'start': 0,\n",
    "    'q': 'heat stress guidlines interventions mitigation strategies' \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d1be7349c94787acff0ccc72944c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##GET TITLE/LINK/AUTHORS OF N*10 ARTICLES\n",
    "\n",
    "#loop through pages for n pages\n",
    "n = 5\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#store results\n",
    "results = pd.DataFrame(columns=['title', 'link', 'author'])\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    params['start'] = i*10\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('div', class_='gs_ri')\n",
    "\n",
    "    #get titles\n",
    "    titles = [article.find('h3', class_='gs_rt').text for article in articles]\n",
    "    #get links\n",
    "    links = [article.find('h3', class_='gs_rt').find('a')['href'] for article in articles]\n",
    "    #get authors\n",
    "    authors = [article.find('div', class_='gs_a').text for article in articles]\n",
    "    \n",
    "    #add to results\n",
    "    results = results._append(pd.DataFrame({'title': titles, 'link': links, 'author': authors}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('article_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BASIC CODE TO SCRAPE ABSTRACT \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_abstract(url):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        abstract = soup.find('blockquote', class_='abstract').text.strip()\n",
    "    \n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [400]>\n"
     ]
    }
   ],
   "source": [
    "link = 'https://www.sciencedirect.com/science/article/pii/S0921800919302976'\n",
    "\n",
    "response = requests.get(link, headers=headers)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE TO DOWNLOAD 1 LINK\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "main_page_url = 'https://sci-hub.ru/'\n",
    "url_to_search = 'https://www.sciencedirect.com/science/article/pii/S0921800919302976'\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    # Open page\n",
    "    driver.get(main_page_url)\n",
    "    time.sleep(2)  # Wait for the page to load (you can decrease this time if need be)\n",
    "\n",
    "    #doi_input = driver.find_element()\n",
    "    # Find search bar -> input URL\n",
    "    search_bar = driver.find_element(by=By.ID, value='request')\n",
    "    search_bar.send_keys(url_to_search)\n",
    "    search_bar.send_keys(Keys.RETURN)  # To submit the search query\n",
    "    time.sleep(1)  # Wait for a bit again\n",
    "\n",
    "    # Find download link\n",
    "    download_button = driver.find_element(by=By.XPATH, value=\"//button[@onclick]\")\n",
    "    onclick_attribute = download_button.get_attribute('onclick')\n",
    "\n",
    "    # Extract URL from the onclick attribute\n",
    "    # Assuming the format is location.href='URL'\n",
    "    pdf_url = onclick_attribute.split(\"'\")[1]\n",
    "    \n",
    "    download_link = 'https://' + urlparse(main_page_url).hostname + pdf_url\n",
    "  \n",
    "    # Download the PDF using requests\n",
    "    response = requests.get(download_link)\n",
    "\n",
    "    with open('pdf.pdf', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle the exception, print an error message, etc.\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping function\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time\n",
    "import requests\n",
    "\n",
    "def pdf_download(url_to_search, file_name):\n",
    "    driver = webdriver.Chrome() #initialize web-finder\n",
    "    main_page_url = 'https://sci-hub.ru/' #open sci-hub\n",
    "    driver.get(main_page_url)\n",
    "    time.sleep(1)\n",
    "    # Find search bar -> input URL\n",
    "    search_bar = driver.find_element(by=By.ID, value='request')\n",
    "    search_bar.send_keys(url_to_search)\n",
    "    search_bar.send_keys(Keys.RETURN)  # To submit the search query\n",
    "    time.sleep(1)\n",
    "    \n",
    "    try:\n",
    "        # Find download link\n",
    "        download_button = driver.find_element(by=By.XPATH, value=\"//button[@onclick]\")\n",
    "        onclick_attribute = download_button.get_attribute('onclick')\n",
    "    except:\n",
    "        driver.quit()\n",
    "        return url_to_search\n",
    "\n",
    "    \n",
    "    # Extract URL from the onclick attribute\n",
    "    # Assuming the format is location.href='URL'\n",
    "    pdf_url = onclick_attribute.split(\"'\")[1]\n",
    "    download_link = 'https://sci-hub.ru' + pdf_url\n",
    "    # Download the PDF using requests\n",
    "    response = requests.get(download_link)\n",
    "    with open(f'pdfs/{file_name}.pdf', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##to call on all links !!! DO NOT RERUN, IT WILL SET LIST BACK TO NONE\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data = pd.read_csv('article_info.csv')\n",
    "title = data['title']\n",
    "links = data['link']\n",
    "author = ['author'] \n",
    "\n",
    "\n",
    "def file_exists(file_path):\n",
    "    return os.path.exists(file_path)\n",
    "\n",
    "list_error_links = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF already downloaded\n",
      "ERROR FOR https://www.jstage.jst.go.jp/article/indhealth/51/1/51_2012-0089/_article/-char/ja/\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "ERROR FOR https://journals.uni-lj.si/aas/article/view/12783\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "ERROR FOR https://jamanetwork.com/journals/jamainternalmedicine/article-abstract/413470\n",
      "PDF already downloaded\n",
      "ERROR FOR https://researchportal.port.ac.uk/files/3398222/Heat_acclimation.pdf\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "ERROR FOR https://books.google.com/books?hl=en&lr=&id=D9pNrH6U0oYC&oi=fnd&pg=PR7&dq=heat+stress+guidlines+interventions+mitigation+strategies&ots=Uwmb8JjYGG&sig=mP62VMTdXLhQoeU9Ji7EMZhBSFU\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "ERROR FOR https://www.sciencedirect.com/science/article/pii/S0362028X22003738\n",
      "ERROR FOR https://books.google.com/books?hl=en&lr=&id=_hE9CgAAQBAJ&oi=fnd&pg=PT21&dq=heat+stress+guidlines+interventions+mitigation+strategies&ots=INK3UBmE65&sig=qDxU6vHvhMpVqZ_Ba2d37U1y77k\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n"
     ]
    }
   ],
   "source": [
    "#function to call on all links\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    pdf_file_path = f'pdfs/{index}.pdf'\n",
    "    if file_exists(pdf_file_path) is True:\n",
    "        print(\"PDF already downloaded\")\n",
    "    else:\n",
    "        if row['link'] in list_error_links:\n",
    "            print('error already encountered')\n",
    "        else:\n",
    "            url_returned = pdf_download(row['link'], index)  #(links[index], index)\n",
    "            if url_returned is not None:\n",
    "                list_error_links.append(url_returned)\n",
    "                print(f'ERROR FOR {url_returned}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\users\\gur levy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.17.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\Gur Levy\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\celia\\\\OneDrive - UvA\\\\semester 3\\\\DE\\\\trashmasters\\\\scholar scraper\\\\pdfs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m isfile, join\n\u001b[0;32m      5\u001b[0m mypath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcelia\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOneDrive - UvA\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msemester 3\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDE\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtrashmasters\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mscholar scraper\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mpdfs\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m pdf_folder_content \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmypath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m isfile(join(mypath, f))]\n\u001b[0;32m      9\u001b[0m all_pdf_texts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pdf \u001b[38;5;129;01min\u001b[39;00m pdf_folder_content:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\celia\\\\OneDrive - UvA\\\\semester 3\\\\DE\\\\trashmasters\\\\scholar scraper\\\\pdfs'"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "mypath = 'C:\\\\Users\\\\celia\\\\OneDrive - UvA\\\\semester 3\\\\DE\\\\trashmasters\\\\scholar scraper\\\\pdfs'\n",
    "\n",
    "pdf_folder_content = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "all_pdf_texts = []\n",
    "\n",
    "for pdf in pdf_folder_content:\n",
    "    print(f'Analyzing {pdf}...')\n",
    "\t\n",
    "    reader = PdfReader(f'{mypath}/{pdf}')\n",
    "\n",
    "    n_pages = len(reader.pages)\n",
    "\t\n",
    "    print(f'{pdf} has {n_pages} pages!')\n",
    "\n",
    "    all_text = []\n",
    "\n",
    "    for i in range(0,n_pages):  \n",
    "      page_text = reader.pages[i].extract_text()\n",
    "      all_text.append(page_text)\n",
    "\n",
    "    result_string = ' '.join(all_text)\n",
    "    all_pdf_texts.append(result_string)\n",
    "\n",
    "##SWITCH TO A DICTIONARY INSTEAD OF A LIST\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "additional_stopwords = ['et', 'al','b', 'fw', 'e'] \n",
    "\n",
    "def preprocess_text(text):\n",
    "    #YOUR CODE HERE\n",
    "\n",
    "    # removes special characters and punctuation (STEP 1: Text cleaning)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text) \n",
    "    # remove additional white spaces (STEP 1: Text cleaning)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() \n",
    "\n",
    "    #put text in lowercases (STEP 2: Case normalization)\n",
    "    text = text.lower()\n",
    "\n",
    "    #STEP 3: tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    #STEP 4: removing stopwords:\n",
    "    stopwords_list = set(stopwords.words('english')).union(additional_stopwords)\n",
    "    tokens = [token for token in tokens if token not in stopwords_list]\n",
    "\n",
    "    #STEP 5: lemmeatization: \n",
    "    lemmatized_words = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = [preprocess_text(text) for text in all_pdf_texts]\n",
    "\n",
    "corpus = corpora.Dictionary(processed_text)\n",
    "doc_term_matrix = [corpus.doc2bow(text) for text in processed_text]\n",
    "\n",
    "num_topics = 45\n",
    "\n",
    "lda_model = LdaModel(doc_term_matrix, id2word = corpus, num_topics = num_topics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common topics and the number of instances are:\n",
      "Topic 16: Count 6\n",
      "Topic 44: Count 4\n",
      "Topic 9: Count 3\n",
      "Topic 29: Count 3\n",
      "Topic 2: Count 2\n",
      "Topic 43: Count 2\n",
      "Topic 31: Count 1\n",
      "Topic 21: Count 1\n",
      "Topic 38: Count 1\n",
      "Topic 6: Count 1\n",
      "Topic 10: Count 1\n",
      "Topic 5: Count 1\n",
      "Most used words in Topic 16: waste, plastic, household, recycling, food, economy, product, collection, system, material\n",
      "Most used words in Topic 44: waste, recycling, household, plastic, collection, product, ewaste, study, system, business\n",
      "Most used words in Topic 9: waste, plastic, recycling, material, business, household, food, study, packaging, korea\n",
      "Most used words in Topic 29: waste, household, food, recycling, consumer, product, plastic, collection, study, material\n",
      "Most used words in Topic 2: waste, food, product, recycling, consumer, ewaste, household, study, collection, system\n",
      "Most used words in Topic 43: waste, recycling, system, product, ewaste, household, study, consumer, food, management\n",
      "Most used words in Topic 31: waste, household, plastic, food, recycling, material, sector, system, study, product\n",
      "Most used words in Topic 21: waste, collection, recycling, household, cost, product, consumer, land, system, study\n",
      "Most used words in Topic 38: waste, food, product, household, consumer, system, cost, study, recycling, collection\n",
      "Most used words in Topic 6: waste, ewaste, consumer, food, recycling, product, household, collection, behavior, found\n",
      "Most used words in Topic 10: waste, food, consumer, product, recycling, study, household, collection, ewaste, system\n",
      "Most used words in Topic 5: waste, household, recycling, food, system, product, study, consumer, ewaste, plastic\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "document_topics = [lda_model.get_document_topics(doc) for doc in doc_term_matrix]\n",
    "all_topics = [topic[0] for topics in document_topics for topic in topics]\n",
    "\n",
    "topics_nb = Counter(all_topics)\n",
    "\n",
    "most_nb_topics = [topic for topic, count in topics_nb.most_common()]\n",
    "print(\"The most common topics and the number of instances are:\")\n",
    "for topic, x in topics_nb.most_common():\n",
    "    print(f\"Topic {topic +1}: Count {x}\")\n",
    "for topic in most_nb_topics:\n",
    "    topics_words = lda_model.show_topic(topic)\n",
    "    most_words = [word for word, _ in topics_words]\n",
    "    print(f\"Most used words in Topic {topic +1}: {', '.join(most_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the optimal number of topic for this model is: 45\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Function to compute coherence score\n",
    "def compute_coherence(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    # YOUR CODE HERE\n",
    "    coherence_values = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        lda_model = LdaModel(corpus = corpus, id2word = dictionary, num_topics = num_topics, random_state = 100)\n",
    "\n",
    "        coherence_model_lda = CoherenceModel(model = lda_model, texts = texts, dictionary = dictionary, coherence = 'c_v')\n",
    "        coherence_values.append(coherence_model_lda.get_coherence())\n",
    "    \n",
    "    return coherence_values\n",
    "\n",
    "coherence_values = compute_coherence(dictionary=corpus, corpus= doc_term_matrix, texts=processed_text, limit=50, start = 5, step=5)\n",
    "\n",
    "optimal_nb_topics = (coherence_values.index(max(coherence_values)) + 1) * 5\n",
    "print(f\"the optimal number of topic for this model is: {optimal_nb_topics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
