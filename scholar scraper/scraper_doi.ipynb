{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOOGLE SCHOLAR SEARCH OF KEY WORDS\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url= 'https://scholar.google.com/scholar'\n",
    "\n",
    "#set headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \\\n",
    "    AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36' \n",
    "    #english\n",
    "}\n",
    "\n",
    "params = {\n",
    "    #english\n",
    "    'hl': 'en',\n",
    "\n",
    "    'start': 0,\n",
    "    'q': 'circular economy waste household' # maybe add 'trash' 'garbage' 'individual'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60be3d84f3044dcb828ba0ee428b78aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# extract doi's from url\n",
    "def extract_doi_from_webpage(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            doi_elements = soup.find_all(string=re.compile(r'\\b(10\\.\\d{4,}(?:\\.\\d+)*/\\S+)\\b'))\n",
    "            \n",
    "            if doi_elements:\n",
    "                dois = []\n",
    "                for element in doi_elements:\n",
    "                    match = re.findall(r'\\b(10\\.\\d{4,}(?:\\.\\d+)*/\\S+)\\b', str(element))\n",
    "                    if match:\n",
    "                        #added 'https://doi.org/' in front so that iit can be callable\n",
    "                        dois.extend([f\"https://doi.org/{doi}\" for doi in match])\n",
    "                \n",
    "                return dois \n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return None\n",
    "\n",
    "# loop through pages for n pages\n",
    "n = 10\n",
    "\n",
    "results = pd.DataFrame(columns=['titles', 'links', 'authors', 'dois'])\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    params['start'] = i * 10\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('div', class_='gs_ri')\n",
    "\n",
    "    # get titles\n",
    "    titles = [article.find('h3', class_='gs_rt').text for article in articles]\n",
    "    titles = [title[:title.find('[') - 1] if title.find('[') != -1 else title for title in titles]\n",
    "    #get links\n",
    "    links = [article.find('h3', class_='gs_rt').find('a')['href'] for article in articles]\n",
    "    #get authors\n",
    "    authors = [article.find('div', class_='gs_a').text for article in articles]\n",
    "\n",
    "    # get doi, if it is None, keep link ^ as url of article \n",
    "    dois = [extract_doi_from_webpage(link) for link in links]\n",
    "    dois_links = [doi[0] if doi else None for doi in dois]\n",
    "    final_links = [doi if doi else link for doi, link in zip(dois_links, links)]\n",
    "\n",
    "    \n",
    "    results = pd.concat([results, pd.DataFrame({'titles': titles, 'links': final_links, 'authors': authors, 'dois': dois_links})])\n",
    "\n",
    "results.to_csv('article_info3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                titles  \\\n",
      "0   [HTML][HTML] Consumers in a circular economy: ...   \n",
      "1   [HTML][HTML] Circular economy and household e-...   \n",
      "2   [HTML][HTML] Household organic waste: Integrat...   \n",
      "3   [HTML][HTML] Waste prevention, energy recovery...   \n",
      "4   [HTML][HTML] Potential for circular economy in...   \n",
      "..                                                ...   \n",
      "5   Towards a circular economy: A case study of wa...   \n",
      "6   Assessing the Outcomes of Circular Economy and...   \n",
      "7   [PDF][PDF] Effective solid waste management in...   \n",
      "8   [HTML][HTML] Co-production in solid waste mana...   \n",
      "9   [HTML][HTML] Waste treatment company decision-...   \n",
      "\n",
      "                                                links  \\\n",
      "0   https://www.sciencedirect.com/science/article/...   \n",
      "1   https://www.sciencedirect.com/science/article/...   \n",
      "2   https://www.sciencedirect.com/science/article/...   \n",
      "3   https://www.sciencedirect.com/science/article/...   \n",
      "4   https://www.sciencedirect.com/science/article/...   \n",
      "..                                                ...   \n",
      "5              https://www.mdpi.com/2413-8851/2/4/118   \n",
      "6   https://aptikom-journal.id/itsdi/article/view/609   \n",
      "7   http://www.procedia-esem.eu/pdf/issues/2017/no...   \n",
      "8   https://link.springer.com/article/10.1007/s113...   \n",
      "9   https://www.sciencedirect.com/science/article/...   \n",
      "\n",
      "                                              authors  \n",
      "0   D Nainggolan, AB Pedersen, S Smed, KH Zemo… - ...  \n",
      "1   D Sengupta, I Ilankoon, KD Kang, MN Chong - Mi...  \n",
      "2   É Celestino, A Carvalho, JM Palma-Oliveira - J...  \n",
      "3   I de Sadeleer, H Brattebø, P Callewaert - Reso...  \n",
      "4   K Parajuly, H Wenzel - Journal of Cleaner Prod...  \n",
      "..                                                ...  \n",
      "5   Z Allam, DS Jones - Urban Science, 2018 - mdpi...  \n",
      "6   E Dollan, BDK Ramadhan - IAIC Transactions on ...  \n",
      "7   O Oyelola, I Ajiboshin, J Okewole - … Science,...  \n",
      "8   OB Ezeudu, TC Oraelosi, JC Agunwamba… - … Scie...  \n",
      "9   S Snellinx, J Van Meensel, S Farahbakhsh… - Jo...  \n",
      "\n",
      "[100 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(results.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic scraping function\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time\n",
    "import requests\n",
    "\n",
    "def pdf_download(url_to_search, file_name):\n",
    "    driver = webdriver.Chrome() #initialize web-finder\n",
    "    main_page_url = 'https://sci-hub.ru/' #open sci-hub\n",
    "    driver.get(main_page_url)\n",
    "    time.sleep(1)\n",
    "    # Find search bar -> input URL\n",
    "    search_bar = driver.find_element(by=By.ID, value='request')\n",
    "    search_bar.send_keys(url_to_search)\n",
    "    search_bar.send_keys(Keys.RETURN)  # To submit the search query\n",
    "    time.sleep(1)\n",
    "    \n",
    "    try:\n",
    "        # Find download link\n",
    "        download_button = driver.find_element(by=By.XPATH, value=\"//button[@onclick]\")\n",
    "        onclick_attribute = download_button.get_attribute('onclick')\n",
    "    except:\n",
    "        driver.quit()\n",
    "        return url_to_search\n",
    "\n",
    "    \n",
    "    # Extract URL from the onclick attribute\n",
    "    # Assuming the format is location.href='URL'\n",
    "    pdf_url = onclick_attribute.split(\"'\")[1]\n",
    "    download_link = 'https://sci-hub.ru' + pdf_url\n",
    "    # Download the PDF using requests\n",
    "    response = requests.get(download_link)\n",
    "    with open(f'pdfs/{file_name}.pdf', 'wb') as file:\n",
    "       file.write(response.content)\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF already downloaded\n",
      "Error encountered for https://www.sciencedirect.com/science/article/pii/S0892687522002710\n",
      "Error encountered for https://www.sciencedirect.com/science/article/pii/S0959652622040185\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "Error encountered for http://ppeu.stu.cn.ua/article/view/126051\n",
      "Error encountered for https://doi.org/10.1108/MEQ-04-2021-0089/full/html\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "Error encountered for https://doi.org/10.3390/su141610116\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "Error encountered for https://onlinelibrary.wiley.com/doi/abs/10.1002/bse.3365\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "Error encountered for https://doi.org/10.55983/empjcs.v1i2.68\n",
      "Error encountered for http://www.eemj.icpm.tuiasi.ro/pdfs/vol20/no1/Full/9_125_Tripa_20.pdf\n",
      "PDF already downloaded\n",
      "Error encountered for https://www.sciencedirect.com/science/article/pii/S0892687523001681\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "Error encountered for https://doi.org/10.3390/en14196164\n",
      "Error encountered for https://doi.org/10.1108/IJLM-03-2022-0100/full/html\n",
      "Error encountered for https://doi.org/10.3390/su132313467\n",
      "PDF already downloaded\n",
      "Error encountered for https://www.sciencedirect.com/science/article/pii/S0959652622027160\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "Error encountered for https://doi.org/10.3390/su15129431\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "Error encountered for https://doi.org/10.3390/su14010480\n",
      "Error encountered for https://doi.org/10.1007/978-981-15-8510-4_20\",\"Page\":\"chapter\",\"page\":{\"attributes\":{\"environment\":\"live\"}},\"Country\":\"NL\",\"japan\":false,\"doi\":\"10.1007-978-981-15-8510-4_20\",\"Keywords\":\"Circular\n",
      "Error encountered for https://www.sciencedirect.com/science/article/pii/S0959652622018868\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "Error encountered for https://doi.org/10.3390/su15075843\n",
      "Error encountered for https://doi.org/10.1453/jest.v3i2.854\n",
      "Error encountered for https://www.sciencedirect.com/science/article/pii/S0956053X22003944\n",
      "Error encountered for https://doi.org/10.3390/su14159565\n",
      "Error encountered for https://doi.org/10.3390/en15041269\n",
      "Error encountered for https://doi.org/10.1088/1755-1315/1094/1/012001\n",
      "Error encountered for https://www.academia.edu/download/32547802/A_global_redesign_-_shaping_the_circular_economy.pdf\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "Error encountered for https://doi.org/10.5276/JSWTM.2009.209\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "Error encountered for https://www.sciencedirect.com/science/article/pii/S2666784322000389\n",
      "Error encountered for https://doi.org/10.2861/978568\n",
      "Error encountered for https://doi.org/10.3390/en15072584\n",
      "Error encountered for https://books.google.com/books?hl=en&lr=&id=v5alDwAAQBAJ&oi=fnd&pg=PP10&dq=circular+economy+waste+household&ots=19ijy23z57&sig=4IJpVoT_iiVYcD5jZvIc4xt8Qqk\n",
      "PDF already downloaded\n",
      "Error encountered for https://www.sciencedirect.com/science/article/pii/S0301479723009684\n",
      "PDF already downloaded\n",
      "Error encountered for https://doi.org/10.1007/978-981-15-1052-6_22\",\"Page\":\"chapter\",\"page\":{\"attributes\":{\"environment\":\"live\"}},\"Country\":\"NL\",\"japan\":false,\"doi\":\"10.1007-978-981-15-1052-6_22\",\"Keywords\":\"Vietnam\n",
      "PDF already downloaded\n",
      "Error encountered for https://doi.org/10.1007/s11356-023-29511-8\",\"Page\":\"article\",\"springerJournal\":true,\"page\":{\"attributes\":{\"environment\":\"live\"}},\"Country\":\"NL\",\"japan\":false,\"doi\":\"10.1007-s11356-023-29511-8\",\"Journal\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "Error encountered for https://www.sciencedirect.com/science/article/pii/S0959652622008976\n",
      "Error encountered for https://doi.org/10.1007/s10163-023-01826-1\",\"Page\":\"article\",\"springerJournal\":true,\"page\":{\"attributes\":{\"environment\":\"live\"}},\"Country\":\"NL\",\"japan\":false,\"doi\":\"10.1007-s10163-023-01826-1\",\"Journal\n",
      "PDF already downloaded\n",
      "Error encountered for https://www.ceeol.com/search/article-detail?id=908497\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "Error encountered for https://www.researchgate.net/profile/Saravan-Krishnamurthy/publication/312125734_Circular_Economy_for_Sustainable_Development_in_India/links/5870c6ab08ae8fce491e1d95/Circular-Economy-for-Sustainable-Development-in-India.pdf\n",
      "PDF already downloaded\n",
      "Error encountered for https://doi.org/10.1007/s10479-023-05386-3\",\"Page\":\"article\",\"springerJournal\":true,\"page\":{\"attributes\":{\"environment\":\"live\"}},\"Country\":\"NL\",\"japan\":false,\"doi\":\"10.1007-s10479-023-05386-3\",\"Journal\n",
      "Error encountered for https://doi.org/10.3390/su14010020\n",
      "PDF already downloaded\n",
      "Error encountered for https://www.sciencedirect.com/science/article/pii/B9780128216644000030\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "Error encountered for http://www.procedia-esem.eu/pdf/issues/2017/no3/20_Oyelola_17.pdf\n",
      "PDF already downloaded\n",
      "Error encountered for https://www.sciencedirect.com/science/article/pii/S095965262103849X\n"
     ]
    }
   ],
   "source": [
    "#LOOP THROUGH URLS to downalod by calling the pdf_download function\n",
    "import os\n",
    "data = pd.read_csv('article_info3.csv')\n",
    "list_error_links = []  # Define an empty list to store error links if encountered\n",
    "\n",
    "# Function to check if a file exists at a given path\n",
    "def file_exists(file_path):\n",
    "    return os.path.exists(file_path)\n",
    "\n",
    "# Loop through each row in the DataFrame 'data'\n",
    "for index, row in data.iterrows():\n",
    "    pdf_file_path = f'pdfs/{index}.pdf'  # Define the path for the PDF file\n",
    "    if file_exists(pdf_file_path):\n",
    "        print(\"PDF already downloaded\")\n",
    "    else:\n",
    "        if row['links'] in list_error_links:\n",
    "            print('Error already encountered')\n",
    "        else:\n",
    "            url_returned = pdf_download(row['links'], '{index}.pdf')  \n",
    "            if url_returned is not None:\n",
    "                list_error_links.append(url_returned)\n",
    "                print(f'Error encountered for {url_returned}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
