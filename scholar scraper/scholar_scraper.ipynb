{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url= 'https://scholar.google.com/scholar'\n",
    "\n",
    "#set headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \\\n",
    "    AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36' \n",
    "    #english\n",
    "}\n",
    "\n",
    "params = {\n",
    "    #english\n",
    "    'hl': 'en',\n",
    "\n",
    "    'start': 0,\n",
    "    'q': 'circular economy waste household' \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4480dfa53c34273911c0661bf90da3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
=======
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gur Levy\\trashmasters-11\\scholar scraper\\scholar_scraper.ipynb Cell 2\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gur%20Levy/trashmasters-11/scholar%20scraper/scholar_scraper.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#store results\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gur%20Levy/trashmasters-11/scholar%20scraper/scholar_scraper.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m results \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlink\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mauthor\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Gur%20Levy/trashmasters-11/scholar%20scraper/scholar_scraper.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39;49m(n)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gur%20Levy/trashmasters-11/scholar%20scraper/scholar_scraper.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     params[\u001b[39m'\u001b[39m\u001b[39mstart\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m i\u001b[39m*\u001b[39m\u001b[39m10\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gur%20Levy/trashmasters-11/scholar%20scraper/scholar_scraper.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url, headers\u001b[39m=\u001b[39mheaders, params\u001b[39m=\u001b[39mparams)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\notebook.py:233\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m unit_scale \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[0;32m    232\u001b[0m total \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39m*\u001b[39m unit_scale \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal\n\u001b[1;32m--> 233\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstatus_printer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp, total, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdesc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mncols)\n\u001b[0;32m    234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer\u001b[39m.\u001b[39mpbar \u001b[39m=\u001b[39m proxy(\u001b[39mself\u001b[39m)\n\u001b[0;32m    235\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplayed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[39m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39m# if not total:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[39m# Prepare IPython progress bar\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[39mif\u001b[39;00m IProgress \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# #187 #451 #558 #872\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[0;32m    109\u001b[0m \u001b[39mif\u001b[39;00m total:\n\u001b[0;32m    110\u001b[0m     pbar \u001b[39m=\u001b[39m IProgress(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39mtotal)\n",
      "\u001b[1;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
>>>>>>> 4eb54b2e7a87dd683a7d97e982cc42e93a89dc60
    }
   ],
   "source": [
    "##GET TITLE/LINK/AUTHORS OF N*10 ARTICLES\n",
    "\n",
    "#loop through pages for n pages\n",
    "n = 10\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#store results\n",
    "results = pd.DataFrame(columns=['title', 'link', 'author'])\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    params['start'] = i*10\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('div', class_='gs_ri')\n",
    "\n",
    "    #get titles\n",
    "    titles = [article.find('h3', class_='gs_rt').text for article in articles]\n",
    "    #get links\n",
    "    links = [article.find('h3', class_='gs_rt').find('a')['href'] for article in articles]\n",
    "    #get authors\n",
    "    authors = [article.find('div', class_='gs_a').text for article in articles]\n",
    "    \n",
    "    #add to results\n",
    "    results = results._append(pd.DataFrame({'title': titles, 'link': links, 'author': authors}))\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 38,
=======
   "execution_count": 6,
>>>>>>> 4eb54b2e7a87dd683a7d97e982cc42e93a89dc60
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('article_info.csv')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 7,
>>>>>>> 4eb54b2e7a87dd683a7d97e982cc42e93a89dc60
   "metadata": {},
   "outputs": [],
   "source": [
    "## BASIC CODE TO SCRAPE ABSTRACT \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_abstract(url):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        abstract = soup.find('blockquote', class_='abstract').text.strip()\n",
    "    \n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 8,
>>>>>>> 4eb54b2e7a87dd683a7d97e982cc42e93a89dc60
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [403]>\n"
     ]
    }
   ],
   "source": [
    "link = 'https://www.sciencedirect.com/science/article/pii/S0921800919302976'\n",
    "\n",
    "response = requests.get(link, headers=headers)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x000002523626A5F0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Gur Levy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\std.py\", line 1149, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\Users\\Gur Levy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\notebook.py\", line 278, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n"
     ]
    }
   ],
>>>>>>> 4eb54b2e7a87dd683a7d97e982cc42e93a89dc60
   "source": [
    "## CODE TO DOWNLOAD 1 LINK\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "main_page_url = 'https://sci-hub.ru/'\n",
    "url_to_search = 'https://www.sciencedirect.com/science/article/pii/S0921800919302976'\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    # Open page\n",
    "    driver.get(main_page_url)\n",
    "    time.sleep(2)  # Wait for the page to load (you can decrease this time if need be)\n",
    "\n",
    "    #doi_input = driver.find_element()\n",
    "    # Find search bar -> input URL\n",
    "    search_bar = driver.find_element(by=By.ID, value='request')\n",
    "    search_bar.send_keys(url_to_search)\n",
    "    search_bar.send_keys(Keys.RETURN)  # To submit the search query\n",
    "    time.sleep(1)  # Wait for a bit again\n",
    "\n",
    "    # Find download link\n",
    "    download_button = driver.find_element(by=By.XPATH, value=\"//button[@onclick]\")\n",
    "    onclick_attribute = download_button.get_attribute('onclick')\n",
    "\n",
    "    # Extract URL from the onclick attribute\n",
    "    # Assuming the format is location.href='URL'\n",
    "    pdf_url = onclick_attribute.split(\"'\")[1]\n",
    "    \n",
    "    download_link = 'https://' + urlparse(main_page_url).hostname + pdf_url\n",
    "  \n",
    "    # Download the PDF using requests\n",
    "    response = requests.get(download_link)\n",
    "\n",
    "    with open('pdf.pdf', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle the exception, print an error message, etc.\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 25,
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[name=\"doi\"]\"}\n  (Session info: chrome=119.0.6045.160); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x009C72A3+45731]\n\t(No symbol) [0x00952D51]\n\t(No symbol) [0x0084880D]\n\t(No symbol) [0x0087B940]\n\t(No symbol) [0x0087BE0B]\n\t(No symbol) [0x008AD1F2]\n\t(No symbol) [0x00898024]\n\t(No symbol) [0x008AB7A2]\n\t(No symbol) [0x00897DD6]\n\t(No symbol) [0x008731F6]\n\t(No symbol) [0x0087439D]\n\tGetHandleVerifier [0x00CD0716+3229462]\n\tGetHandleVerifier [0x00D184C8+3523784]\n\tGetHandleVerifier [0x00D1214C+3498316]\n\tGetHandleVerifier [0x00A51680+611968]\n\t(No symbol) [0x0095CCCC]\n\t(No symbol) [0x00958DF8]\n\t(No symbol) [0x00958F1D]\n\t(No symbol) [0x0094B2C7]\n\tBaseThreadInitThunk [0x75FBFCC9+25]\n\tRtlGetAppContainerNamedObjectPath [0x77277C6E+286]\n\tRtlGetAppContainerNamedObjectPath [0x77277C3E+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gur Levy\\trashmasters-6\\scholar scraper\\scholar_scraper.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gur%20Levy/trashmasters-6/scholar%20scraper/scholar_scraper.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m2\u001b[39m)  \u001b[39m# Wait for the page to load (you can decrease this time if need be)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gur%20Levy/trashmasters-6/scholar%20scraper/scholar_scraper.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Find DOI input field and submit the DOI\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Gur%20Levy/trashmasters-6/scholar%20scraper/scholar_scraper.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m doi_input \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39;49mfind_element(by\u001b[39m=\u001b[39;49mBy\u001b[39m.\u001b[39;49mNAME, value\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdoi\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gur%20Levy/trashmasters-6/scholar%20scraper/scholar_scraper.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m doi_input\u001b[39m.\u001b[39msend_keys(doi)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gur%20Levy/trashmasters-6/scholar%20scraper/scholar_scraper.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m doi_input\u001b[39m.\u001b[39msend_keys(Keys\u001b[39m.\u001b[39mRETURN)  \u001b[39m# To submit the DOI\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:738\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    735\u001b[0m     by \u001b[39m=\u001b[39m By\u001b[39m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    736\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[name=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 738\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mFIND_ELEMENT, {\u001b[39m\"\u001b[39;49m\u001b[39musing\u001b[39;49m\u001b[39m\"\u001b[39;49m: by, \u001b[39m\"\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m\"\u001b[39;49m: value})[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:344\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    342\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_executor\u001b[39m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m--> 344\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_handler\u001b[39m.\u001b[39;49mcheck_response(response)\n\u001b[0;32m    345\u001b[0m     response[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unwrap_value(response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    346\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[39m=\u001b[39m value[\u001b[39m\"\u001b[39m\u001b[39malert\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[39m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[name=\"doi\"]\"}\n  (Session info: chrome=119.0.6045.160); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x009C72A3+45731]\n\t(No symbol) [0x00952D51]\n\t(No symbol) [0x0084880D]\n\t(No symbol) [0x0087B940]\n\t(No symbol) [0x0087BE0B]\n\t(No symbol) [0x008AD1F2]\n\t(No symbol) [0x00898024]\n\t(No symbol) [0x008AB7A2]\n\t(No symbol) [0x00897DD6]\n\t(No symbol) [0x008731F6]\n\t(No symbol) [0x0087439D]\n\tGetHandleVerifier [0x00CD0716+3229462]\n\tGetHandleVerifier [0x00D184C8+3523784]\n\tGetHandleVerifier [0x00D1214C+3498316]\n\tGetHandleVerifier [0x00A51680+611968]\n\t(No symbol) [0x0095CCCC]\n\t(No symbol) [0x00958DF8]\n\t(No symbol) [0x00958F1D]\n\t(No symbol) [0x0094B2C7]\n\tBaseThreadInitThunk [0x75FBFCC9+25]\n\tRtlGetAppContainerNamedObjectPath [0x77277C6E+286]\n\tRtlGetAppContainerNamedObjectPath [0x77277C3E+238]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import requests\n",
    "\n",
    "main_page_url = 'https://sci-hub.ru/'\n",
    "doi = 'https://doi.org/10.1016/j.ecolecon.2019.106402'\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    # Open page\n",
    "    driver.get(main_page_url)\n",
    "    time.sleep(2)  # Wait for the page to load (you can decrease this time if need be)\n",
    "\n",
    "    # Find DOI input field and submit the DOI\n",
    "    doi_input = driver.find_element(by=By.ID, value='request')\n",
    "    doi_input.send_keys(doi)\n",
    "    doi_input.send_keys(Keys.RETURN)  # To submit the DOI\n",
    "    time.sleep(1)  # Wait for a bit again\n",
    "\n",
    "    # Find download link\n",
    "    download_button = driver.find_element(by=By.XPATH, value=\"//button[@onclick]\")\n",
    "    onclick_attribute = download_button.get_attribute('onclick')\n",
    "    #download_link_element = driver.find_element(by=By.XPATH, value=\"//ul[@class='mb-4']//li/a[contains(@href, 'scimag')]\")\n",
    "    \n",
    "    download_link_element = WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.XPATH, \"//ul[@class='mb-4']//li/a[contains(@href, 'scimag')]\"))\n",
    ")\n",
    "    download_link = download_link_element.get_attribute('href')\n",
    "    # Extract URL from the onclick attribute\n",
    "    # Assuming the format is location.href='URL'\n",
    "    #pdf_url = onclick_attribute.split(\"'\")[1]\n",
    "    \n",
    "    #download_link = 'https:' + pdf_url\n",
    "\n",
    "    # Download the PDF using requests\n",
    "    response = requests.get(download_link)\n",
    "    with open('pdf.pdf', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> 4eb54b2e7a87dd683a7d97e982cc42e93a89dc60
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping function\n",
    "\n",
<<<<<<< HEAD
=======
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time\n",
    "import requests\n",
    "\n",
    "main_page_url = 'https://sci-hub.ru/'\n",
    "url_to_search = 'https://www.sciencedirect.com/science/article/pii/S0959652617320346'\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    # Open page\n",
    "    driver.get(main_page_url)\n",
    "    time.sleep(1)  # Wait for the page to load (you can decrease this time if need be)\n",
    "\n",
    "    # Find search bar -> input URL\n",
    "    search_bar = driver.find_element(by=By.ID, value='request')\n",
    "    search_bar.send_keys(url_to_search)\n",
    "    search_bar.send_keys(Keys.RETURN)  # To submit the search query\n",
    "    time.sleep(1)  # Wait for a bit again\n",
    "\n",
    "    # Find download link\n",
    "    download_button = driver.find_element(by=By.XPATH, value=\"//button[@onclick]\")\n",
    "    onclick_attribute = download_button.get_attribute('onclick')\n",
    "\n",
    "    # Extract URL from the onclick attribute\n",
    "    # Assuming the format is location.href='URL'\n",
    "    pdf_url = onclick_attribute.split(\"'\")[1]\n",
    "    \n",
    "    download_link = 'https:' + pdf_url\n",
    "\n",
    "    # Download the PDF using requests\n",
    "    response = requests.get(download_link)\n",
    "    with open('/pdfs/pdf.pdf', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "except:\n",
    "    driver.quit()\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "    main_page_url = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
>>>>>>> 4eb54b2e7a87dd683a7d97e982cc42e93a89dc60
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time\n",
    "import requests\n",
    "\n",
    "def pdf_download(url_to_search, file_name):\n",
    "    driver = webdriver.Chrome() #initialize web-finder\n",
    "    main_page_url = 'https://sci-hub.ru/' #open sci-hub\n",
    "    driver.get(main_page_url)\n",
    "    time.sleep(1)\n",
    "    # Find search bar -> input URL\n",
    "    search_bar = driver.find_element(by=By.ID, value='request')\n",
    "    search_bar.send_keys(url_to_search)\n",
    "    search_bar.send_keys(Keys.RETURN)  # To submit the search query\n",
    "    time.sleep(1)\n",
    "    \n",
    "    try:\n",
    "        # Find download link\n",
    "        download_button = driver.find_element(by=By.XPATH, value=\"//button[@onclick]\")\n",
    "        onclick_attribute = download_button.get_attribute('onclick')\n",
    "    except:\n",
    "        driver.quit()\n",
    "        return url_to_search\n",
    "\n",
    "    \n",
    "    # Extract URL from the onclick attribute\n",
    "    # Assuming the format is location.href='URL'\n",
    "    pdf_url = onclick_attribute.split(\"'\")[1]\n",
    "    download_link = 'https://sci-hub.ru' + pdf_url\n",
    "    # Download the PDF using requests\n",
    "    response = requests.get(download_link)\n",
    "    with open(f'pdfs/{file_name}.pdf', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##to call on all links !!! DO NOT RERUN, IT WILL SET LIST BACK TO NONE\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data = pd.read_csv('article_info.csv')\n",
    "title = data['title']\n",
    "links = data['link']\n",
    "author = ['author'] \n",
    "\n",
    "\n",
    "def file_exists(file_path):\n",
    "    return os.path.exists(file_path)\n",
    "\n",
    "list_error_links = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
=======
   "execution_count": null,
>>>>>>> 4eb54b2e7a87dd683a7d97e982cc42e93a89dc60
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF already downloaded\n",
      "error already encountered\n",
      "error already encountered\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "error already encountered\n",
      "error already encountered\n",
      "error already encountered\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "error already encountered\n",
      "PDF already downloaded\n",
      "error already encountered\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "error already encountered\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "error already encountered\n",
      "error already encountered\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "error already encountered\n",
      "error already encountered\n",
      "PDF already downloaded\n",
      "ERROR FOR https://www.sciencedirect.com/science/article/pii/S0892687523001681\n",
      "ERROR FOR https://www.tandfonline.com/doi/abs/10.1080/10962247.2016.1229700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Service.__del__ at 0x0000027F4CB417E0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 185, in __del__\n",
      "    self.stop()\n",
      "  File \"c:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 146, in stop\n",
      "    self.send_remote_shutdown_command()\n",
      "  File \"c:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 126, in send_remote_shutdown_command\n",
      "    request.urlopen(f\"{self.service_url}/shutdown\")\n",
      "  File \"c:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py\", line 216, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "  File \"c:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py\", line 519, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"c:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py\", line 536, in _open\n",
      "    result = self._call_chain(self.handle_open, protocol, protocol +\n",
      "  File \"c:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py\", line 496, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"c:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py\", line 1377, in http_open\n",
      "    return self.do_open(http.client.HTTPConnection, req)\n",
      "  File \"c:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py\", line 1348, in do_open\n",
      "    h.request(req.get_method(), req.selector, req.data, headers,\n",
      "  File \"c:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py\", line 1282, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"c:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py\", line 1328, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"c:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py\", line 1277, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"c:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py\", line 1037, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"c:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py\", line 975, in send\n",
      "    self.connect()\n",
      "  File \"c:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py\", line 941, in connect\n",
      "    self.sock = self._create_connection(\n",
      "  File \"c:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py\", line 833, in create_connection\n",
      "    sock.connect(sa)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR FOR https://www.sciencedirect.com/science/article/pii/S0959652620340920\n",
      "ERROR FOR https://www.mdpi.com/1996-1073/14/19/6164\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=119.0.6045.199)\nStacktrace:\n\tGetHandleVerifier [0x00007FF6B27182B2+55298]\n\t(No symbol) [0x00007FF6B2685E02]\n\t(No symbol) [0x00007FF6B25405AB]\n\t(No symbol) [0x00007FF6B2520038]\n\t(No symbol) [0x00007FF6B25A6BC7]\n\t(No symbol) [0x00007FF6B25BA15F]\n\t(No symbol) [0x00007FF6B25A1E83]\n\t(No symbol) [0x00007FF6B257670A]\n\t(No symbol) [0x00007FF6B2577964]\n\tGetHandleVerifier [0x00007FF6B2A90AAB+3694587]\n\tGetHandleVerifier [0x00007FF6B2AE728E+4048862]\n\tGetHandleVerifier [0x00007FF6B2ADF173+4015811]\n\tGetHandleVerifier [0x00007FF6B27B47D6+695590]\n\t(No symbol) [0x00007FF6B2690CE8]\n\t(No symbol) [0x00007FF6B268CF34]\n\t(No symbol) [0x00007FF6B268D062]\n\t(No symbol) [0x00007FF6B267D3A3]\n\tBaseThreadInitThunk [0x00007FF9C041257D+29]\n\tRtlUserThreadStart [0x00007FF9C15EAA58+40]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\celia\\OneDrive - UvA\\semester 3\\DE\\trashmasters\\scholar scraper\\scholar_scraper.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/celia/OneDrive%20-%20UvA/semester%203/DE/trashmasters/scholar%20scraper/scholar_scraper.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39merror already encountered\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/celia/OneDrive%20-%20UvA/semester%203/DE/trashmasters/scholar%20scraper/scholar_scraper.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/celia/OneDrive%20-%20UvA/semester%203/DE/trashmasters/scholar%20scraper/scholar_scraper.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     url_returned \u001b[39m=\u001b[39m pdf_download(row[\u001b[39m'\u001b[39;49m\u001b[39mlink\u001b[39;49m\u001b[39m'\u001b[39;49m], index)  \u001b[39m#(links[index], index)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/celia/OneDrive%20-%20UvA/semester%203/DE/trashmasters/scholar%20scraper/scholar_scraper.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mif\u001b[39;00m url_returned \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/celia/OneDrive%20-%20UvA/semester%203/DE/trashmasters/scholar%20scraper/scholar_scraper.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         list_error_links\u001b[39m.\u001b[39mappend(url_returned)\n",
      "\u001b[1;32mc:\\Users\\celia\\OneDrive - UvA\\semester 3\\DE\\trashmasters\\scholar scraper\\scholar_scraper.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/celia/OneDrive%20-%20UvA/semester%203/DE/trashmasters/scholar%20scraper/scholar_scraper.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/celia/OneDrive%20-%20UvA/semester%203/DE/trashmasters/scholar%20scraper/scholar_scraper.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Find search bar -> input URL\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/celia/OneDrive%20-%20UvA/semester%203/DE/trashmasters/scholar%20scraper/scholar_scraper.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m search_bar \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39;49mfind_element(by\u001b[39m=\u001b[39;49mBy\u001b[39m.\u001b[39;49mID, value\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrequest\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/celia/OneDrive%20-%20UvA/semester%203/DE/trashmasters/scholar%20scraper/scholar_scraper.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m search_bar\u001b[39m.\u001b[39msend_keys(url_to_search)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/celia/OneDrive%20-%20UvA/semester%203/DE/trashmasters/scholar%20scraper/scholar_scraper.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m search_bar\u001b[39m.\u001b[39msend_keys(Keys\u001b[39m.\u001b[39mRETURN)  \u001b[39m# To submit the search query\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:738\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    735\u001b[0m     by \u001b[39m=\u001b[39m By\u001b[39m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    736\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[name=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 738\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mFIND_ELEMENT, {\u001b[39m\"\u001b[39;49m\u001b[39musing\u001b[39;49m\u001b[39m\"\u001b[39;49m: by, \u001b[39m\"\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m\"\u001b[39;49m: value})[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:344\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    342\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_executor\u001b[39m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m--> 344\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_handler\u001b[39m.\u001b[39;49mcheck_response(response)\n\u001b[0;32m    345\u001b[0m     response[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unwrap_value(response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    346\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[39m=\u001b[39m value[\u001b[39m\"\u001b[39m\u001b[39malert\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[39m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=119.0.6045.199)\nStacktrace:\n\tGetHandleVerifier [0x00007FF6B27182B2+55298]\n\t(No symbol) [0x00007FF6B2685E02]\n\t(No symbol) [0x00007FF6B25405AB]\n\t(No symbol) [0x00007FF6B2520038]\n\t(No symbol) [0x00007FF6B25A6BC7]\n\t(No symbol) [0x00007FF6B25BA15F]\n\t(No symbol) [0x00007FF6B25A1E83]\n\t(No symbol) [0x00007FF6B257670A]\n\t(No symbol) [0x00007FF6B2577964]\n\tGetHandleVerifier [0x00007FF6B2A90AAB+3694587]\n\tGetHandleVerifier [0x00007FF6B2AE728E+4048862]\n\tGetHandleVerifier [0x00007FF6B2ADF173+4015811]\n\tGetHandleVerifier [0x00007FF6B27B47D6+695590]\n\t(No symbol) [0x00007FF6B2690CE8]\n\t(No symbol) [0x00007FF6B268CF34]\n\t(No symbol) [0x00007FF6B268D062]\n\t(No symbol) [0x00007FF6B267D3A3]\n\tBaseThreadInitThunk [0x00007FF9C041257D+29]\n\tRtlUserThreadStart [0x00007FF9C15EAA58+40]\n"
     ]
    }
   ],
   "source": [
    "#function to call on all links\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    pdf_file_path = f'pdfs/{index}.pdf'\n",
    "    if file_exists(pdf_file_path) is True:\n",
    "        print(\"PDF already downloaded\")\n",
    "    else:\n",
    "        if row['link'] in list_error_links:\n",
    "            print('error already encountered')\n",
    "        else:\n",
    "            url_returned = pdf_download(row['link'], index)  #(links[index], index)\n",
    "            if url_returned is not None:\n",
    "                list_error_links.append(url_returned)\n",
    "                print(f'ERROR FOR {url_returned}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-3.17.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading pypdf-3.17.1-py3-none-any.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 277.6/277.6 kB 8.6 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-3.17.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 0.pdf...\n",
      "0.pdf has 12 pages!\n",
      "Analyzing 11.pdf...\n",
      "11.pdf has 12 pages!\n",
      "Analyzing 14.pdf...\n",
      "14.pdf has 11 pages!\n",
      "Analyzing 15.pdf...\n",
      "15.pdf has 6 pages!\n",
      "Analyzing 16.pdf...\n",
      "16.pdf has 11 pages!\n",
      "Analyzing 17.pdf...\n",
      "17.pdf has 8 pages!\n",
      "Analyzing 19.pdf...\n",
      "19.pdf has 18 pages!\n",
      "Analyzing 23.pdf...\n",
      "23.pdf has 36 pages!\n",
      "Analyzing 24.pdf...\n",
      "24.pdf has 16 pages!\n",
      "Analyzing 27.pdf...\n",
      "27.pdf has 19 pages!\n",
      "Analyzing 3.pdf...\n",
      "3.pdf has 9 pages!\n",
      "Analyzing 8.pdf...\n",
      "8.pdf has 14 pages!\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "mypath = 'C:\\\\Users\\\\celia\\\\OneDrive - UvA\\\\semester 3\\\\DE\\\\trashmasters\\\\scholar scraper\\\\pdfs'\n",
    "\n",
    "pdf_folder_content = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "all_pdf_texts = []\n",
    "\n",
    "for pdf in pdf_folder_content:\n",
    "    print(f'Analyzing {pdf}...')\n",
    "\t\n",
    "    reader = PdfReader(f'{mypath}/{pdf}')\n",
    "\n",
    "    n_pages = len(reader.pages)\n",
    "\t\n",
    "    print(f'{pdf} has {n_pages} pages!')\n",
    "\n",
    "    all_text = []\n",
    "\n",
    "    for i in range(0,n_pages):  \n",
    "      page_text = reader.pages[i].extract_text()\n",
    "      all_text.append(page_text)\n",
    "\n",
    "    result_string = ' '.join(all_text)\n",
    "    all_pdf_texts.append(result_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "additional_stopwords = ['et', 'al','b', 'fw', 'e'] \n",
    "\n",
    "def preprocess_text(text):\n",
    "    #YOUR CODE HERE\n",
    "\n",
    "    # removes special characters and punctuation (STEP 1: Text cleaning)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text) \n",
    "    # remove additional white spaces (STEP 1: Text cleaning)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() \n",
    "\n",
    "    #put text in lowercases (STEP 2: Case normalization)\n",
    "    text = text.lower()\n",
    "\n",
    "    #STEP 3: tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    #STEP 4: removing stopwords:\n",
    "    stopwords_list = set(stopwords.words('english')).union(additional_stopwords)\n",
    "    tokens = [token for token in tokens if token not in stopwords_list]\n",
    "\n",
    "    #STEP 5: lemmeatization: \n",
    "    lemmatized_words = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = [preprocess_text(text) for text in all_pdf_texts]\n",
    "\n",
    "corpus = corpora.Dictionary(processed_text)\n",
    "doc_term_matrix = [corpus.doc2bow(text) for text in processed_text]\n",
    "\n",
    "num_topics = 45\n",
    "\n",
    "lda_model = LdaModel(doc_term_matrix, id2word = corpus, num_topics = num_topics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common topics and the number of instances are:\n",
      "Topic 16: Count 6\n",
      "Topic 44: Count 4\n",
      "Topic 9: Count 3\n",
      "Topic 29: Count 3\n",
      "Topic 2: Count 2\n",
      "Topic 43: Count 2\n",
      "Topic 31: Count 1\n",
      "Topic 21: Count 1\n",
      "Topic 38: Count 1\n",
      "Topic 6: Count 1\n",
      "Topic 10: Count 1\n",
      "Topic 5: Count 1\n",
      "Most used words in Topic 16: waste, plastic, household, recycling, food, economy, product, collection, system, material\n",
      "Most used words in Topic 44: waste, recycling, household, plastic, collection, product, ewaste, study, system, business\n",
      "Most used words in Topic 9: waste, plastic, recycling, material, business, household, food, study, packaging, korea\n",
      "Most used words in Topic 29: waste, household, food, recycling, consumer, product, plastic, collection, study, material\n",
      "Most used words in Topic 2: waste, food, product, recycling, consumer, ewaste, household, study, collection, system\n",
      "Most used words in Topic 43: waste, recycling, system, product, ewaste, household, study, consumer, food, management\n",
      "Most used words in Topic 31: waste, household, plastic, food, recycling, material, sector, system, study, product\n",
      "Most used words in Topic 21: waste, collection, recycling, household, cost, product, consumer, land, system, study\n",
      "Most used words in Topic 38: waste, food, product, household, consumer, system, cost, study, recycling, collection\n",
      "Most used words in Topic 6: waste, ewaste, consumer, food, recycling, product, household, collection, behavior, found\n",
      "Most used words in Topic 10: waste, food, consumer, product, recycling, study, household, collection, ewaste, system\n",
      "Most used words in Topic 5: waste, household, recycling, food, system, product, study, consumer, ewaste, plastic\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "document_topics = [lda_model.get_document_topics(doc) for doc in doc_term_matrix]\n",
    "all_topics = [topic[0] for topics in document_topics for topic in topics]\n",
    "\n",
    "topics_nb = Counter(all_topics)\n",
    "\n",
    "most_nb_topics = [topic for topic, count in topics_nb.most_common()]\n",
    "print(\"The most common topics and the number of instances are:\")\n",
    "for topic, x in topics_nb.most_common():\n",
    "    print(f\"Topic {topic +1}: Count {x}\")\n",
    "for topic in most_nb_topics:\n",
    "    topics_words = lda_model.show_topic(topic)\n",
    "    most_words = [word for word, _ in topics_words]\n",
    "    print(f\"Most used words in Topic {topic +1}: {', '.join(most_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the optimal number of topic for this model is: 45\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Function to compute coherence score\n",
    "def compute_coherence(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    # YOUR CODE HERE\n",
    "    coherence_values = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        lda_model = LdaModel(corpus = corpus, id2word = dictionary, num_topics = num_topics, random_state = 100)\n",
    "\n",
    "        coherence_model_lda = CoherenceModel(model = lda_model, texts = texts, dictionary = dictionary, coherence = 'c_v')\n",
    "        coherence_values.append(coherence_model_lda.get_coherence())\n",
    "    \n",
    "    return coherence_values\n",
    "\n",
    "coherence_values = compute_coherence(dictionary=corpus, corpus= doc_term_matrix, texts=processed_text, limit=50, start = 5, step=5)\n",
    "\n",
    "optimal_nb_topics = (coherence_values.index(max(coherence_values)) + 1) * 5\n",
    "print(f\"the optimal number of topic for this model is: {optimal_nb_topics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
