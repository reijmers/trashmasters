{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url= 'https://scholar.google.com/scholar'\n",
    "\n",
    "#set headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \\\n",
    "    AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36' \n",
    "    #english\n",
    "}\n",
    "\n",
    "\n",
    "params = {\n",
    "    #english\n",
    "    'hl': 'en',\n",
    "\n",
    "    'start': 0,\n",
    "    'q': 'circular economy waste household' \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61110fbe73d947689df1843c7050cc15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#loop through pages for n pages\n",
    "n = 1\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#store results\n",
    "results = pd.DataFrame(columns=['title', 'link', 'author'])\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    params['start'] = i*10\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('div', class_='gs_ri')\n",
    "\n",
    "    #get titles\n",
    "    titles = [article.find('h3', class_='gs_rt').text for article in articles]\n",
    "    #get links\n",
    "    links = [article.find('h3', class_='gs_rt').find('a')['href'] for article in articles]\n",
    "    #get authors\n",
    "    authors = [article.find('div', class_='gs_a').text for article in articles]\n",
    "    \n",
    "    #add to results\n",
    "    results = results._append(pd.DataFrame({'title': titles, 'link': links, 'author': authors}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_abstract(url):\n",
    "    \"\"\"\n",
    "    Fetch the abstract from the provided arXiv URL using XPath.\n",
    "    \n",
    "    Parameters:\n",
    "    - url (str): The URL of the arXiv paper.\n",
    "\n",
    "    Returns:\n",
    "    - str: The abstract of the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        abstract = soup.find('blockquote', class_='abstract').text.strip()\n",
    "    \n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [403]>\n"
     ]
    }
   ],
   "source": [
    "link = 'https://www.sciencedirect.com/science/article/pii/S0921800919302976'\n",
    "\n",
    "response = requests.get(link, headers=headers)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "main_page_url = 'https://sci-hub.ru/'\n",
    "url_to_search = 'https://www.sciencedirect.com/science/article/pii/S0921800919302976'\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    # Open page\n",
    "    driver.get(main_page_url)\n",
    "    time.sleep(2)  # Wait for the page to load (you can decrease this time if need be)\n",
    "\n",
    "    #doi_input = driver.find_element()\n",
    "    # Find search bar -> input URL\n",
    "    search_bar = driver.find_element(by=By.ID, value='request')\n",
    "    search_bar.send_keys(url_to_search)\n",
    "    search_bar.send_keys(Keys.RETURN)  # To submit the search query\n",
    "    time.sleep(1)  # Wait for a bit again\n",
    "\n",
    "    # Find download link\n",
    "    download_button = driver.find_element(by=By.XPATH, value=\"//button[@onclick]\")\n",
    "    onclick_attribute = download_button.get_attribute('onclick')\n",
    "\n",
    "    # Extract URL from the onclick attribute\n",
    "    # Assuming the format is location.href='URL'\n",
    "    pdf_url = onclick_attribute.split(\"'\")[1]\n",
    "    \n",
    "    download_link = 'https://' + urlparse(main_page_url).hostname + pdf_url\n",
    "  \n",
    "    # Download the PDF using requests\n",
    "    response = requests.get(download_link)\n",
    "\n",
    "    with open('pdf.pdf', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle the exception, print an error message, etc.\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//ul[@class='mb-4']//li/a[contains(@href, 'scimag')]\"}\n  (Session info: chrome=119.0.6045.160); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x011672A3+45731]\n\t(No symbol) [0x010F2D51]\n\t(No symbol) [0x00FE880D]\n\t(No symbol) [0x0101B940]\n\t(No symbol) [0x0101BE0B]\n\t(No symbol) [0x0104D1F2]\n\t(No symbol) [0x01038024]\n\t(No symbol) [0x0104B7A2]\n\t(No symbol) [0x01037DD6]\n\t(No symbol) [0x010131F6]\n\t(No symbol) [0x0101439D]\n\tGetHandleVerifier [0x01470716+3229462]\n\tGetHandleVerifier [0x014B84C8+3523784]\n\tGetHandleVerifier [0x014B214C+3498316]\n\tGetHandleVerifier [0x011F1680+611968]\n\t(No symbol) [0x010FCCCC]\n\t(No symbol) [0x010F8DF8]\n\t(No symbol) [0x010F8F1D]\n\t(No symbol) [0x010EB2C7]\n\tBaseThreadInitThunk [0x76417BA9+25]\n\tRtlInitializeExceptionChain [0x7710BD3B+107]\n\tRtlClearBits [0x7710BCBF+191]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\celia\\OneDrive - UvA\\semester 3\\DE\\trashmasters\\scholar scraper\\scholar_scraper.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/celia/OneDrive%20-%20UvA/semester%203/DE/trashmasters/scholar%20scraper/scholar_scraper.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)  \u001b[39m# Wait for a bit again\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/celia/OneDrive%20-%20UvA/semester%203/DE/trashmasters/scholar%20scraper/scholar_scraper.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m# Find download link\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/celia/OneDrive%20-%20UvA/semester%203/DE/trashmasters/scholar%20scraper/scholar_scraper.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m#download_button = driver.find_element(by=By.XPATH, value=\"//button[@onclick]\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/celia/OneDrive%20-%20UvA/semester%203/DE/trashmasters/scholar%20scraper/scholar_scraper.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m#onclick_attribute = download_button.get_attribute('onclick')\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/celia/OneDrive%20-%20UvA/semester%203/DE/trashmasters/scholar%20scraper/scholar_scraper.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     download_link_element \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39;49mfind_element(by\u001b[39m=\u001b[39;49mBy\u001b[39m.\u001b[39;49mXPATH, value\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m//ul[@class=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmb-4\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m]//li/a[contains(@href, \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mscimag\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m)]\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/celia/OneDrive%20-%20UvA/semester%203/DE/trashmasters/scholar%20scraper/scholar_scraper.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     download_link_element \u001b[39m=\u001b[39m WebDriverWait(driver, \u001b[39m10\u001b[39m)\u001b[39m.\u001b[39muntil(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/celia/OneDrive%20-%20UvA/semester%203/DE/trashmasters/scholar%20scraper/scholar_scraper.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     EC\u001b[39m.\u001b[39mpresence_of_element_located((By\u001b[39m.\u001b[39mXPATH, \u001b[39m\"\u001b[39m\u001b[39m//ul[@class=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmb-4\u001b[39m\u001b[39m'\u001b[39m\u001b[39m]//li/a[contains(@href, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mscimag\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)]\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/celia/OneDrive%20-%20UvA/semester%203/DE/trashmasters/scholar%20scraper/scholar_scraper.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/celia/OneDrive%20-%20UvA/semester%203/DE/trashmasters/scholar%20scraper/scholar_scraper.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     download_link \u001b[39m=\u001b[39m download_link_element\u001b[39m.\u001b[39mget_attribute(\u001b[39m'\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:738\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    735\u001b[0m     by \u001b[39m=\u001b[39m By\u001b[39m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    736\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[name=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 738\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mFIND_ELEMENT, {\u001b[39m\"\u001b[39;49m\u001b[39musing\u001b[39;49m\u001b[39m\"\u001b[39;49m: by, \u001b[39m\"\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m\"\u001b[39;49m: value})[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:344\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    342\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_executor\u001b[39m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m--> 344\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_handler\u001b[39m.\u001b[39;49mcheck_response(response)\n\u001b[0;32m    345\u001b[0m     response[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unwrap_value(response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    346\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\celia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[39m=\u001b[39m value[\u001b[39m\"\u001b[39m\u001b[39malert\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[39m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//ul[@class='mb-4']//li/a[contains(@href, 'scimag')]\"}\n  (Session info: chrome=119.0.6045.160); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x011672A3+45731]\n\t(No symbol) [0x010F2D51]\n\t(No symbol) [0x00FE880D]\n\t(No symbol) [0x0101B940]\n\t(No symbol) [0x0101BE0B]\n\t(No symbol) [0x0104D1F2]\n\t(No symbol) [0x01038024]\n\t(No symbol) [0x0104B7A2]\n\t(No symbol) [0x01037DD6]\n\t(No symbol) [0x010131F6]\n\t(No symbol) [0x0101439D]\n\tGetHandleVerifier [0x01470716+3229462]\n\tGetHandleVerifier [0x014B84C8+3523784]\n\tGetHandleVerifier [0x014B214C+3498316]\n\tGetHandleVerifier [0x011F1680+611968]\n\t(No symbol) [0x010FCCCC]\n\t(No symbol) [0x010F8DF8]\n\t(No symbol) [0x010F8F1D]\n\t(No symbol) [0x010EB2C7]\n\tBaseThreadInitThunk [0x76417BA9+25]\n\tRtlInitializeExceptionChain [0x7710BD3B+107]\n\tRtlClearBits [0x7710BCBF+191]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import requests\n",
    "\n",
    "main_page_url = 'https://annas-archive.org/'\n",
    "doi = 'https://doi.org/10.1016/j.ecolecon.2019.106402'\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    # Open page\n",
    "    driver.get(main_page_url)\n",
    "    time.sleep(2)  # Wait for the page to load (you can decrease this time if need be)\n",
    "\n",
    "    # Find DOI input field and submit the DOI\n",
    "    doi_input = driver.find_element(by=By.NAME, value='doi')\n",
    "    doi_input.send_keys(doi)\n",
    "    doi_input.send_keys(Keys.RETURN)  # To submit the DOI\n",
    "    time.sleep(1)  # Wait for a bit again\n",
    "\n",
    "    # Find download link\n",
    "    #download_button = driver.find_element(by=By.XPATH, value=\"//button[@onclick]\")\n",
    "    #onclick_attribute = download_button.get_attribute('onclick')\n",
    "    download_link_element = driver.find_element(by=By.XPATH, value=\"//ul[@class='mb-4']//li/a[contains(@href, 'scimag')]\")\n",
    "    \n",
    "    download_link_element = WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.XPATH, \"//ul[@class='mb-4']//li/a[contains(@href, 'scimag')]\"))\n",
    ")\n",
    "    download_link = download_link_element.get_attribute('href')\n",
    "    # Extract URL from the onclick attribute\n",
    "    # Assuming the format is location.href='URL'\n",
    "    #pdf_url = onclick_attribute.split(\"'\")[1]\n",
    "    \n",
    "    #download_link = 'https:' + pdf_url\n",
    "\n",
    "    # Download the PDF using requests\n",
    "    response = requests.get(download_link)\n",
    "    with open('pdf.pdf', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
