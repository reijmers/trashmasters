{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOOGLE SCHOLAR SEARCH OF KEY WORDS\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url= 'https://scholar.google.com/scholar'\n",
    "\n",
    "#set headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \\\n",
    "    AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36' \n",
    "    #english\n",
    "}\n",
    "\n",
    "params = {\n",
    "    #english\n",
    "    'hl': 'en',\n",
    "\n",
    "    'start': 0,\n",
    "    'q': 'circular economy waste household' # maybe add 'trash' 'garbage' 'individual'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11d383f9747445da6de2b1d1b3e567d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##GET TITLE/LINK/AUTHORS OF N*X ARTICLES\n",
    "\n",
    "#loop through pages for n pages\n",
    "n = 10\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#store results\n",
    "results = pd.DataFrame(columns = ['titles', 'links', 'authors'])\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    params['start'] = i*10\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('div', class_='gs_ri')\n",
    "\n",
    "    #get titles\n",
    "    titles = [article.find('h3', class_='gs_rt').text for article in articles]\n",
    "    #locate and strip the '[HTML]' or '[PDF]' in the titles\n",
    "    titles = [title[:title.find('[')-1] if title.find('[') != -1 else title for title in titles]\n",
    "\n",
    "    #get links\n",
    "    links = [article.find('h3', class_='gs_rt').find('a')['href'] for article in articles]\n",
    "    #get authors\n",
    "    authors = [article.find('div', class_='gs_a').text for article in articles]\n",
    "    \n",
    "    #add to results\n",
    "    results = results._append(pd.DataFrame({'titles' : titles, 'links' : links, 'authors' : authors}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                titles  \\\n",
      "0   [HTML][HTML] Consumers in a circular economy: ...   \n",
      "1   [HTML][HTML] Circular economy and household e-...   \n",
      "2   [HTML][HTML] Household organic waste: Integrat...   \n",
      "3   [HTML][HTML] Waste prevention, energy recovery...   \n",
      "4   [HTML][HTML] Potential for circular economy in...   \n",
      "..                                                ...   \n",
      "5   Towards a circular economy: A case study of wa...   \n",
      "6   Assessing the Outcomes of Circular Economy and...   \n",
      "7   [PDF][PDF] Effective solid waste management in...   \n",
      "8   [HTML][HTML] Co-production in solid waste mana...   \n",
      "9   [HTML][HTML] Waste treatment company decision-...   \n",
      "\n",
      "                                                links  \\\n",
      "0   https://www.sciencedirect.com/science/article/...   \n",
      "1   https://www.sciencedirect.com/science/article/...   \n",
      "2   https://www.sciencedirect.com/science/article/...   \n",
      "3   https://www.sciencedirect.com/science/article/...   \n",
      "4   https://www.sciencedirect.com/science/article/...   \n",
      "..                                                ...   \n",
      "5              https://www.mdpi.com/2413-8851/2/4/118   \n",
      "6   https://aptikom-journal.id/itsdi/article/view/609   \n",
      "7   http://www.procedia-esem.eu/pdf/issues/2017/no...   \n",
      "8   https://link.springer.com/article/10.1007/s113...   \n",
      "9   https://www.sciencedirect.com/science/article/...   \n",
      "\n",
      "                                              authors  \n",
      "0   D Nainggolan, AB Pedersen, S Smed, KH Zemo… - ...  \n",
      "1   D Sengupta, I Ilankoon, KD Kang, MN Chong - Mi...  \n",
      "2   É Celestino, A Carvalho, JM Palma-Oliveira - J...  \n",
      "3   I de Sadeleer, H Brattebø, P Callewaert - Reso...  \n",
      "4   K Parajuly, H Wenzel - Journal of Cleaner Prod...  \n",
      "..                                                ...  \n",
      "5   Z Allam, DS Jones - Urban Science, 2018 - mdpi...  \n",
      "6   E Dollan, BDK Ramadhan - IAIC Transactions on ...  \n",
      "7   O Oyelola, I Ajiboshin, J Okewole - … Science,...  \n",
      "8   OB Ezeudu, TC Oraelosi, JC Agunwamba… - … Scie...  \n",
      "9   S Snellinx, J Van Meensel, S Farahbakhsh… - Jo...  \n",
      "\n",
      "[100 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(results.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('info_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION EXCTRACT DOIS\n",
    "import re\n",
    "def extract_doi_from_webpage(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            doi_elements = soup.find_all(string=re.compile(r'\\b(10\\.\\d{4,}(?:\\.\\d+)*/\\S+)\\b'))\n",
    "            \n",
    "            if doi_elements:\n",
    "                dois = []\n",
    "                for element in doi_elements:\n",
    "                    match = re.findall(r'\\b(10\\.\\d{4,}(?:\\.\\d+)*/\\S+)\\b', str(element))\n",
    "                    if match:\n",
    "                        dois.extend(match)\n",
    "                \n",
    "                return dois\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"Failed to fetch content from {url}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error occurred while fetching content from {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca24f2c5c6f4251af6f7a93216a778a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# extract doi's from url\n",
    "def extract_doi_from_webpage(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            doi_elements = soup.find_all(string=re.compile(r'\\b(10\\.\\d{4,}(?:\\.\\d+)*/\\S+)\\b'))\n",
    "            \n",
    "            if doi_elements:\n",
    "                dois = []\n",
    "                for element in doi_elements:\n",
    "                    match = re.findall(r'\\b(10\\.\\d{4,}(?:\\.\\d+)*/\\S+)\\b', str(element))\n",
    "                    if match:\n",
    "                        #added 'https://doi.org/' in front so that iit can be callable\n",
    "                        dois.extend([f\"https://doi.org/{doi}\" for doi in match])\n",
    "                \n",
    "                return dois \n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return None\n",
    "\n",
    "# loop through pages for n pages\n",
    "n = 10\n",
    "\n",
    "results = pd.DataFrame(columns=['titles', 'links', 'authors', 'dois'])\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    params['start'] = i * 10\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('div', class_='gs_ri')\n",
    "\n",
    "    # get titles\n",
    "    titles = [article.find('h3', class_='gs_rt').text for article in articles]\n",
    "    titles = [title[:title.find('[') - 1] if title.find('[') != -1 else title for title in titles]\n",
    "    #get links\n",
    "    links = [article.find('h3', class_='gs_rt').find('a')['href'] for article in articles]\n",
    "    #get authors\n",
    "    authors = [article.find('div', class_='gs_a').text for article in articles]\n",
    "\n",
    "    # get doi, if it is None, keep link ^ as url of article \n",
    "    dois = [extract_doi_from_webpage(link) for link in links]\n",
    "    dois_links = [doi[0] if doi else None for doi in dois]\n",
    "    final_links = [doi if doi else link for doi, link in zip(dois_links, links)]\n",
    "\n",
    "    \n",
    "    results = pd.concat([results, pd.DataFrame({'titles': titles, 'links': final_links, 'authors': authors, 'dois': dois_links})])\n",
    "\n",
    "results.to_csv('article_info3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                titles  \\\n",
      "0   [HTML][HTML] Consumers in a circular economy: ...   \n",
      "1   [HTML][HTML] Circular economy and household e-...   \n",
      "2   [HTML][HTML] Household organic waste: Integrat...   \n",
      "3   [HTML][HTML] Waste prevention, energy recovery...   \n",
      "4   [HTML][HTML] Potential for circular economy in...   \n",
      "..                                                ...   \n",
      "5   Towards a circular economy: A case study of wa...   \n",
      "6   Assessing the Outcomes of Circular Economy and...   \n",
      "7   [PDF][PDF] Effective solid waste management in...   \n",
      "8   [HTML][HTML] Co-production in solid waste mana...   \n",
      "9   [HTML][HTML] Waste treatment company decision-...   \n",
      "\n",
      "                                                links  \\\n",
      "0   https://www.sciencedirect.com/science/article/...   \n",
      "1   https://www.sciencedirect.com/science/article/...   \n",
      "2   https://www.sciencedirect.com/science/article/...   \n",
      "3   https://www.sciencedirect.com/science/article/...   \n",
      "4   https://www.sciencedirect.com/science/article/...   \n",
      "..                                                ...   \n",
      "5             https://doi.org/10.3390/urbansci2040118   \n",
      "6             https://doi.org/10.34306/itsdi.v5i1.609   \n",
      "7   http://www.procedia-esem.eu/pdf/issues/2017/no...   \n",
      "8   https://doi.org/10.1007/s11356-021-14471-8\",\"P...   \n",
      "9   https://www.sciencedirect.com/science/article/...   \n",
      "\n",
      "                                              authors  \\\n",
      "0   D Nainggolan, AB Pedersen, S Smed, KH Zemo… - ...   \n",
      "1   D Sengupta, I Ilankoon, KD Kang, MN Chong - Mi...   \n",
      "2   É Celestino, A Carvalho, JM Palma-Oliveira - J...   \n",
      "3   I de Sadeleer, H Brattebø, P Callewaert - Reso...   \n",
      "4   K Parajuly, H Wenzel - Journal of Cleaner Prod...   \n",
      "..                                                ...   \n",
      "5   Z Allam, DS Jones - Urban Science, 2018 - mdpi...   \n",
      "6   E Dollan, BDK Ramadhan - IAIC Transactions on ...   \n",
      "7   O Oyelola, I Ajiboshin, J Okewole - … Science,...   \n",
      "8   OB Ezeudu, TC Oraelosi, JC Agunwamba… - … Scie...   \n",
      "9   S Snellinx, J Van Meensel, S Farahbakhsh… - Jo...   \n",
      "\n",
      "                                                 dois  \n",
      "0                                                None  \n",
      "1                                                None  \n",
      "2                                                None  \n",
      "3                                                None  \n",
      "4                                                None  \n",
      "..                                                ...  \n",
      "5             https://doi.org/10.3390/urbansci2040118  \n",
      "6             https://doi.org/10.34306/itsdi.v5i1.609  \n",
      "7                                                None  \n",
      "8   https://doi.org/10.1007/s11356-021-14471-8\",\"P...  \n",
      "9                                                None  \n",
      "\n",
      "[100 rows x 4 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(results.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO DOWNLOAD AS PDFS USING DOIS\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "def download_pdfs(dois, file_name):\n",
    "    driver = webdriver.Chrome()  # Initialize web-finder\n",
    "    main_page_url = 'https://sci-hub.ru/'  # Open sci-hub\n",
    "\n",
    "    download_links = []  # List to store download links\n",
    "    \n",
    "    for doi in dois:\n",
    "        try:\n",
    "            driver.get(main_page_url)\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Find search bar -> input DOI\n",
    "            search_bar = driver.find_element(by=By.ID, value='request')\n",
    "            search_bar.send_keys(doi)\n",
    "            search_bar.send_keys(Keys.RETURN)  # To submit the search query\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Find the download button\n",
    "            download_button = driver.find_element(by=By.XPATH, value=\"//button[@onclick]\")\n",
    "            onclick_attribute = download_button.get_attribute('onclick')\n",
    "            pdf_url = onclick_attribute.split(\"'\")[1]\n",
    "            download_link = 'https://sci-hub.ru' + pdf_url\n",
    "            download_links.append(download_link)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while downloading from Sci-Hub: {e}\")\n",
    "    \n",
    "        driver.quit()\n",
    "        return download_links\n",
    "    # Extract URL from the onclick attribute\n",
    "    # Assuming the format is location.href='URL'\n",
    "    pdf_url = onclick_attribute.split(\"'\")[1]\n",
    "    download_link = 'https://sci-hub.ru' + pdf_url\n",
    "    # Download the PDF using requests\n",
    "    response = requests.get(download_link)\n",
    "    with open(f'pdfs/{file_name}.pdf', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF already downloaded\n",
      "An error occurred while downloading from Sci-Hub: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//button[@onclick]\"}\n",
      "  (Session info: chrome=120.0.6099.71); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF772A24D02+56194]\n",
      "\t(No symbol) [0x00007FF7729904B2]\n",
      "\t(No symbol) [0x00007FF7728376AA]\n",
      "\t(No symbol) [0x00007FF7728816D0]\n",
      "\t(No symbol) [0x00007FF7728817EC]\n",
      "\t(No symbol) [0x00007FF7728C4D77]\n",
      "\t(No symbol) [0x00007FF7728A5EBF]\n",
      "\t(No symbol) [0x00007FF7728C2786]\n",
      "\t(No symbol) [0x00007FF7728A5C23]\n",
      "\t(No symbol) [0x00007FF772874A45]\n",
      "\t(No symbol) [0x00007FF772875AD4]\n",
      "\tGetHandleVerifier [0x00007FF772D9D5BB+3695675]\n",
      "\tGetHandleVerifier [0x00007FF772DF6197+4059159]\n",
      "\tGetHandleVerifier [0x00007FF772DEDF63+4025827]\n",
      "\tGetHandleVerifier [0x00007FF772ABF029+687785]\n",
      "\t(No symbol) [0x00007FF77299B508]\n",
      "\t(No symbol) [0x00007FF772997564]\n",
      "\t(No symbol) [0x00007FF7729976E9]\n",
      "\t(No symbol) [0x00007FF772988094]\n",
      "\tBaseThreadInitThunk [0x00007FFDCCF87344+20]\n",
      "\tRtlUserThreadStart [0x00007FFDCDD626B1+33]\n",
      "\n",
      "Error encountered for []\n",
      "An error occurred while downloading from Sci-Hub: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//button[@onclick]\"}\n",
      "  (Session info: chrome=120.0.6099.71); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF772A24D02+56194]\n",
      "\t(No symbol) [0x00007FF7729904B2]\n",
      "\t(No symbol) [0x00007FF7728376AA]\n",
      "\t(No symbol) [0x00007FF7728816D0]\n",
      "\t(No symbol) [0x00007FF7728817EC]\n",
      "\t(No symbol) [0x00007FF7728C4D77]\n",
      "\t(No symbol) [0x00007FF7728A5EBF]\n",
      "\t(No symbol) [0x00007FF7728C2786]\n",
      "\t(No symbol) [0x00007FF7728A5C23]\n",
      "\t(No symbol) [0x00007FF772874A45]\n",
      "\t(No symbol) [0x00007FF772875AD4]\n",
      "\tGetHandleVerifier [0x00007FF772D9D5BB+3695675]\n",
      "\tGetHandleVerifier [0x00007FF772DF6197+4059159]\n",
      "\tGetHandleVerifier [0x00007FF772DEDF63+4025827]\n",
      "\tGetHandleVerifier [0x00007FF772ABF029+687785]\n",
      "\t(No symbol) [0x00007FF77299B508]\n",
      "\t(No symbol) [0x00007FF772997564]\n",
      "\t(No symbol) [0x00007FF7729976E9]\n",
      "\t(No symbol) [0x00007FF772988094]\n",
      "\tBaseThreadInitThunk [0x00007FFDCCF87344+20]\n",
      "\tRtlUserThreadStart [0x00007FFDCDD626B1+33]\n",
      "\n",
      "Error encountered for []\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "An error occurred while downloading from Sci-Hub: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//button[@onclick]\"}\n",
      "  (Session info: chrome=120.0.6099.71); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF772A24D02+56194]\n",
      "\t(No symbol) [0x00007FF7729904B2]\n",
      "\t(No symbol) [0x00007FF7728376AA]\n",
      "\t(No symbol) [0x00007FF7728816D0]\n",
      "\t(No symbol) [0x00007FF7728817EC]\n",
      "\t(No symbol) [0x00007FF7728C4D77]\n",
      "\t(No symbol) [0x00007FF7728A5EBF]\n",
      "\t(No symbol) [0x00007FF7728C2786]\n",
      "\t(No symbol) [0x00007FF7728A5C23]\n",
      "\t(No symbol) [0x00007FF772874A45]\n",
      "\t(No symbol) [0x00007FF772875AD4]\n",
      "\tGetHandleVerifier [0x00007FF772D9D5BB+3695675]\n",
      "\tGetHandleVerifier [0x00007FF772DF6197+4059159]\n",
      "\tGetHandleVerifier [0x00007FF772DEDF63+4025827]\n",
      "\tGetHandleVerifier [0x00007FF772ABF029+687785]\n",
      "\t(No symbol) [0x00007FF77299B508]\n",
      "\t(No symbol) [0x00007FF772997564]\n",
      "\t(No symbol) [0x00007FF7729976E9]\n",
      "\t(No symbol) [0x00007FF772988094]\n",
      "\tBaseThreadInitThunk [0x00007FFDCCF87344+20]\n",
      "\tRtlUserThreadStart [0x00007FFDCDD626B1+33]\n",
      "\n",
      "Error encountered for []\n",
      "An error occurred while downloading from Sci-Hub: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=120.0.6099.71)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF772A24D02+56194]\n",
      "\t(No symbol) [0x00007FF7729904B2]\n",
      "\t(No symbol) [0x00007FF7728376AA]\n",
      "\t(No symbol) [0x00007FF772810AFD]\n",
      "\t(No symbol) [0x00007FF7728AC9AB]\n",
      "\t(No symbol) [0x00007FF7728C201F]\n",
      "\t(No symbol) [0x00007FF7728A5C23]\n",
      "\t(No symbol) [0x00007FF772874A45]\n",
      "\t(No symbol) [0x00007FF772875AD4]\n",
      "\tGetHandleVerifier [0x00007FF772D9D5BB+3695675]\n",
      "\tGetHandleVerifier [0x00007FF772DF6197+4059159]\n",
      "\tGetHandleVerifier [0x00007FF772DEDF63+4025827]\n",
      "\tGetHandleVerifier [0x00007FF772ABF029+687785]\n",
      "\t(No symbol) [0x00007FF77299B508]\n",
      "\t(No symbol) [0x00007FF772997564]\n",
      "\t(No symbol) [0x00007FF7729976E9]\n",
      "\t(No symbol) [0x00007FF772988094]\n",
      "\tBaseThreadInitThunk [0x00007FFDCCF87344+20]\n",
      "\tRtlUserThreadStart [0x00007FFDCDD626B1+33]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\socket.py:833\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    832\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m--> 833\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError already encountered\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     url_returned \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_pdfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlinks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpdfs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m url_returned \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m         list_error_links\u001b[38;5;241m.\u001b[39mappend(url_returned)\n",
      "Cell \u001b[1;32mIn[10], line 33\u001b[0m, in \u001b[0;36mdownload_pdfs\u001b[1;34m(dois, file_name)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while downloading from Sci-Hub: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m     \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m download_links\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Extract URL from the onclick attribute\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Assuming the format is location.href='URL'\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:192\u001b[0m, in \u001b[0;36mChromiumDriver.quit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\selenium\\webdriver\\common\\service.py:146\u001b[0m, in \u001b[0;36mService.stop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_remote_shutdown_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\selenium\\webdriver\\common\\service.py:131\u001b[0m, in \u001b[0;36mService.send_remote_shutdown_command\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[1;32m--> 131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_connectable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    133\u001b[0m     sleep(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\selenium\\webdriver\\common\\service.py:120\u001b[0m, in \u001b[0;36mService.is_connectable\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_connectable\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Establishes a socket connection to determine if the service running\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    on the port is accessible.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_connectable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\selenium\\webdriver\\common\\utils.py:101\u001b[0m, in \u001b[0;36mis_connectable\u001b[1;34m(port, host)\u001b[0m\n\u001b[0;32m     99\u001b[0m socket_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 101\u001b[0m     socket_ \u001b[38;5;241m=\u001b[39m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _is_connectable_exceptions:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\socket.py:833\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[0;32m    832\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m--> 833\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[0;32m    835\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#LOOP THROUGH URLS\n",
    "import os\n",
    "data = pd.read_csv('article_info3.csv')\n",
    "list_error_links = []  # Define an empty list to store error links if encountered\n",
    "\n",
    "# Function to check if a file exists at a given path\n",
    "def file_exists(file_path):\n",
    "    return os.path.exists(file_path)\n",
    "\n",
    "# Loop through each row in the DataFrame 'data'\n",
    "for index, row in data.iterrows():\n",
    "    pdf_file_path = f'pdfs/{index}.pdf'  # Define the path for the PDF file\n",
    "    if file_exists(pdf_file_path):\n",
    "        print(\"PDF already downloaded\")\n",
    "    else:\n",
    "        if row['links'] in list_error_links:\n",
    "            print('Error already encountered')\n",
    "        else:\n",
    "            url_returned = download_pdfs(row['links'], file_name='pdfs')  \n",
    "            if url_returned is not None:\n",
    "                list_error_links.append(url_returned)\n",
    "                print(f'Error encountered for {url_returned}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
