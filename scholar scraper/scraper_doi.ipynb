{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOOGLE SCHOLAR SEARCH OF KEY WORDS\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url= 'https://scholar.google.com/scholar'\n",
    "\n",
    "#set headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \\\n",
    "    AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36' \n",
    "    #english\n",
    "}\n",
    "\n",
    "params = {\n",
    "    #english\n",
    "    'hl': 'en',\n",
    "\n",
    "    'start': 0,\n",
    "    'q': 'circular economy waste household' # maybe add 'trash' 'garbage' 'individual'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29d1e6684cd4c818def6a61e149d75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##GET TITLE/LINK/AUTHORS OF N*X ARTICLES\n",
    "\n",
    "#loop through pages for n pages\n",
    "n = 10\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#store results\n",
    "results = pd.DataFrame(columns = ['titles', 'links', 'authors'])\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    params['start'] = i*10\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('div', class_='gs_ri')\n",
    "\n",
    "    #get titles\n",
    "    titles = [article.find('h3', class_='gs_rt').text for article in articles]\n",
    "    #locate and strip the '[HTML]' or '[PDF]' in the titles\n",
    "    titles = [title[:title.find('[')-1] if title.find('[') != -1 else title for title in titles]\n",
    "\n",
    "    #get links\n",
    "    links = [article.find('h3', class_='gs_rt').find('a')['href'] for article in articles]\n",
    "    #get authors\n",
    "    authors = [article.find('div', class_='gs_a').text for article in articles]\n",
    "    \n",
    "    #add to results\n",
    "    results = results._append(pd.DataFrame({'titles' : titles, 'links' : links, 'authors' : authors}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of Empty DataFrame\n",
      "Columns: [titles, links, authors, dois]\n",
      "Index: []>\n"
     ]
    }
   ],
   "source": [
    "print(results.head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: links, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "results.to_csv('articles_info3.csv')\n",
    "print(results['links'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION EXCTRACT DOIS\n",
    "import re\n",
    "def extract_doi_from_webpage(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            doi_elements = soup.find_all(string=re.compile(r'\\b(10\\.\\d{4,}(?:\\.\\d+)*/\\S+)\\b'))\n",
    "            \n",
    "            if doi_elements:\n",
    "                dois = []\n",
    "                for element in doi_elements:\n",
    "                    match = re.findall(r'\\b(10\\.\\d{4,}(?:\\.\\d+)*/\\S+)\\b', str(element))\n",
    "                    if match:\n",
    "                        dois.extend(match)\n",
    "                \n",
    "                return dois\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"Failed to fetch content from {url}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error occurred while fetching content from {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72bd8bd6276d412b8d6aaa727b3e49f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# extract doi's from url\n",
    "def extract_doi_from_webpage(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            doi_elements = soup.find_all(string=re.compile(r'\\b(10\\.\\d{4,}(?:\\.\\d+)*/\\S+)\\b'))\n",
    "            \n",
    "            if doi_elements:\n",
    "                dois = []\n",
    "                for element in doi_elements:\n",
    "                    match = re.findall(r'\\b(10\\.\\d{4,}(?:\\.\\d+)*/\\S+)\\b', str(element))\n",
    "                    if match:\n",
    "                        #added 'https://doi.org/' in front so that iit can be callable\n",
    "                        dois.extend([f\"https://doi.org/{doi}\" for doi in match])\n",
    "                \n",
    "                return dois \n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return None\n",
    "\n",
    "# loop through pages for n pages\n",
    "n = 10\n",
    "\n",
    "results = pd.DataFrame(columns=['titles', 'links', 'authors', 'dois'])\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    params['start'] = i * 10\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('div', class_='gs_ri')\n",
    "\n",
    "    # get titles\n",
    "    titles = [article.find('h3', class_='gs_rt').text for article in articles]\n",
    "    titles = [title[:title.find('[') - 1] if title.find('[') != -1 else title for title in titles]\n",
    "    #get links\n",
    "    links = [article.find('h3', class_='gs_rt').find('a')['href'] for article in articles]\n",
    "    #get authors\n",
    "    authors = [article.find('div', class_='gs_a').text for article in articles]\n",
    "\n",
    "    # get doi, if it is None, keep link ^ as url of article \n",
    "    dois = [extract_doi_from_webpage(link) for link in links]\n",
    "    dois_links = [doi[0] if doi else None for doi in dois]\n",
    "    final_links = [doi if doi else link for doi, link in zip(dois_links, links)]\n",
    "\n",
    "    \n",
    "    results = pd.concat([results, pd.DataFrame({'titles': titles, 'links': final_links, 'authors': authors, 'dois': dois_links})])\n",
    "\n",
    "results.to_csv('article_info3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: links, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "print(results['links'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO DOWNLOAD AS PDFS USING DOIS\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "def download_pdfs(dois, file_name):\n",
    "    driver = webdriver.Chrome()  # Initialize web-finder\n",
    "    main_page_url = 'https://sci-hub.ru/'  # Open sci-hub\n",
    "\n",
    "    download_links = []  # List to store download links\n",
    "    \n",
    "    for doi in dois:\n",
    "        try:\n",
    "            driver.get(main_page_url)\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Find search bar -> input DOI\n",
    "            search_bar = driver.find_element(by=By.ID, value='request')\n",
    "            search_bar.send_keys(doi)\n",
    "            search_bar.send_keys(Keys.RETURN)  # To submit the search query\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Find the download button\n",
    "            download_button = driver.find_element(by=By.XPATH, value=\"//button[@onclick]\")\n",
    "            onclick_attribute = download_button.get_attribute('onclick')\n",
    "            pdf_url = onclick_attribute.split(\"'\")[1]\n",
    "            download_link = 'https://sci-hub.ru' + pdf_url\n",
    "            download_links.append(download_link)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while downloading from Sci-Hub: {e}\")\n",
    "    \n",
    "        driver.quit()\n",
    "        return download_links\n",
    "    # Extract URL from the onclick attribute\n",
    "    # Assuming the format is location.href='URL'\n",
    "    pdf_url = onclick_attribute.split(\"'\")[1]\n",
    "    download_link = 'https://sci-hub.ru' + pdf_url\n",
    "    # Download the PDF using requests\n",
    "    response = requests.get(download_link)\n",
    "    with open(f'pdfs/{file_name}.pdf', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_download(url_to_search, file_name):\n",
    "    driver = webdriver.Chrome() #initialize web-finder\n",
    "    main_page_url = 'https://sci-hub.ru/' #open sci-hub\n",
    "    driver.get(main_page_url)\n",
    "    time.sleep(1)\n",
    "    # Find search bar -> input URL\n",
    "    search_bar = driver.find_element(by=By.ID, value='request')\n",
    "    search_bar.send_keys(url_to_search)\n",
    "    search_bar.send_keys(Keys.RETURN)  # To submit the search query\n",
    "    time.sleep(1)\n",
    "    \n",
    "    try:\n",
    "        # Find download link\n",
    "        download_button = driver.find_element(by=By.XPATH, value=\"//button[@onclick]\")\n",
    "        onclick_attribute = download_button.get_attribute('onclick')\n",
    "    except:\n",
    "        driver.quit()\n",
    "        return url_to_search\n",
    "\n",
    "    \n",
    "    # Extract URL from the onclick attribute\n",
    "    # Assuming the format is location.href='URL'\n",
    "    pdf_url = onclick_attribute.split(\"'\")[1]\n",
    "    download_link = 'https://sci-hub.ru' + pdf_url\n",
    "    # Download the PDF using requests\n",
    "    response = requests.get(download_link)\n",
    "    with open(f'pdfs/{file_name}.pdf', 'wb') as file:\n",
    "       file.write(response.content)\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOOP THROUGH URLS\n",
    "import os\n",
    "data = pd.read_csv('article_info3.csv')\n",
    "list_error_links = []  # Define an empty list to store error links if encountered\n",
    "\n",
    "# Function to check if a file exists at a given path\n",
    "def file_exists(file_path):\n",
    "    return os.path.exists(file_path)\n",
    "\n",
    "# Loop through each row in the DataFrame 'data'\n",
    "for index, row in data.iterrows():\n",
    "    pdf_file_path = f'pdfs/{index}.pdf'  # Define the path for the PDF file\n",
    "    if file_exists(pdf_file_path):\n",
    "        print(\"PDF already downloaded\")\n",
    "    else:\n",
    "        if row['links'] in list_error_links:\n",
    "            print('Error already encountered')\n",
    "        else:\n",
    "            url_returned = download_pdfs(row['links'], file_name='pdfs')  \n",
    "            if url_returned is not None:\n",
    "                list_error_links.append(url_returned)\n",
    "                print(f'Error encountered for {url_returned}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
