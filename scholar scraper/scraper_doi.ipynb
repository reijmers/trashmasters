{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOOGLE SCHOLAR SEARCH OF KEY WORDS\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url= 'https://scholar.google.com/scholar'\n",
    "\n",
    "#set headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \\\n",
    "    AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36' \n",
    "    #english\n",
    "}\n",
    "\n",
    "params = {\n",
    "    #english\n",
    "    'hl': 'en',\n",
    "\n",
    "    'start': 0,\n",
    "    'q': 'circular economy waste household' # maybe add 'trash' 'garbage' 'individual'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11d383f9747445da6de2b1d1b3e567d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##GET TITLE/LINK/AUTHORS OF N*X ARTICLES\n",
    "\n",
    "#loop through pages for n pages\n",
    "n = 10\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#store results\n",
    "results = pd.DataFrame(columns = ['titles', 'links', 'authors'])\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    params['start'] = i*10\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('div', class_='gs_ri')\n",
    "\n",
    "    #get titles\n",
    "    titles = [article.find('h3', class_='gs_rt').text for article in articles]\n",
    "    #locate and strip the '[HTML]' or '[PDF]' in the titles\n",
    "    titles = [title[:title.find('[')-1] if title.find('[') != -1 else title for title in titles]\n",
    "\n",
    "    #get links\n",
    "    links = [article.find('h3', class_='gs_rt').find('a')['href'] for article in articles]\n",
    "    #get authors\n",
    "    authors = [article.find('div', class_='gs_a').text for article in articles]\n",
    "    \n",
    "    #add to results\n",
    "    results = results._append(pd.DataFrame({'titles' : titles, 'links' : links, 'authors' : authors}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                titles  \\\n",
      "0   [HTML][HTML] Consumers in a circular economy: ...   \n",
      "1   [HTML][HTML] Circular economy and household e-...   \n",
      "2   [HTML][HTML] Household organic waste: Integrat...   \n",
      "3   [HTML][HTML] Waste prevention, energy recovery...   \n",
      "4   [HTML][HTML] Potential for circular economy in...   \n",
      "..                                                ...   \n",
      "5   Towards a circular economy: A case study of wa...   \n",
      "6   Assessing the Outcomes of Circular Economy and...   \n",
      "7   [PDF][PDF] Effective solid waste management in...   \n",
      "8   [HTML][HTML] Co-production in solid waste mana...   \n",
      "9   [HTML][HTML] Waste treatment company decision-...   \n",
      "\n",
      "                                                links  \\\n",
      "0   https://www.sciencedirect.com/science/article/...   \n",
      "1   https://www.sciencedirect.com/science/article/...   \n",
      "2   https://www.sciencedirect.com/science/article/...   \n",
      "3   https://www.sciencedirect.com/science/article/...   \n",
      "4   https://www.sciencedirect.com/science/article/...   \n",
      "..                                                ...   \n",
      "5              https://www.mdpi.com/2413-8851/2/4/118   \n",
      "6   https://aptikom-journal.id/itsdi/article/view/609   \n",
      "7   http://www.procedia-esem.eu/pdf/issues/2017/no...   \n",
      "8   https://link.springer.com/article/10.1007/s113...   \n",
      "9   https://www.sciencedirect.com/science/article/...   \n",
      "\n",
      "                                              authors  \n",
      "0   D Nainggolan, AB Pedersen, S Smed, KH Zemo… - ...  \n",
      "1   D Sengupta, I Ilankoon, KD Kang, MN Chong - Mi...  \n",
      "2   É Celestino, A Carvalho, JM Palma-Oliveira - J...  \n",
      "3   I de Sadeleer, H Brattebø, P Callewaert - Reso...  \n",
      "4   K Parajuly, H Wenzel - Journal of Cleaner Prod...  \n",
      "..                                                ...  \n",
      "5   Z Allam, DS Jones - Urban Science, 2018 - mdpi...  \n",
      "6   E Dollan, BDK Ramadhan - IAIC Transactions on ...  \n",
      "7   O Oyelola, I Ajiboshin, J Okewole - … Science,...  \n",
      "8   OB Ezeudu, TC Oraelosi, JC Agunwamba… - … Scie...  \n",
      "9   S Snellinx, J Van Meensel, S Farahbakhsh… - Jo...  \n",
      "\n",
      "[100 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(results.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('info_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION EXCTRACT DOIS\n",
    "import re\n",
    "def extract_doi_from_webpage(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            doi_elements = soup.find_all(string=re.compile(r'\\b(10\\.\\d{4,}(?:\\.\\d+)*/\\S+)\\b'))\n",
    "            \n",
    "            if doi_elements:\n",
    "                dois = []\n",
    "                for element in doi_elements:\n",
    "                    match = re.findall(r'\\b(10\\.\\d{4,}(?:\\.\\d+)*/\\S+)\\b', str(element))\n",
    "                    if match:\n",
    "                        dois.extend(match)\n",
    "                \n",
    "                return dois\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"Failed to fetch content from {url}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error occurred while fetching content from {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca24f2c5c6f4251af6f7a93216a778a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# extract doi's from url\n",
    "def extract_doi_from_webpage(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            doi_elements = soup.find_all(string=re.compile(r'\\b(10\\.\\d{4,}(?:\\.\\d+)*/\\S+)\\b'))\n",
    "            \n",
    "            if doi_elements:\n",
    "                dois = []\n",
    "                for element in doi_elements:\n",
    "                    match = re.findall(r'\\b(10\\.\\d{4,}(?:\\.\\d+)*/\\S+)\\b', str(element))\n",
    "                    if match:\n",
    "                        #added 'https://doi.org/' in front so that iit can be callable\n",
    "                        dois.extend([f\"https://doi.org/{doi}\" for doi in match])\n",
    "                \n",
    "                return dois \n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return None\n",
    "\n",
    "# loop through pages for n pages\n",
    "n = 10\n",
    "\n",
    "results = pd.DataFrame(columns=['titles', 'links', 'authors', 'dois'])\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    params['start'] = i * 10\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('div', class_='gs_ri')\n",
    "\n",
    "    # get titles\n",
    "    titles = [article.find('h3', class_='gs_rt').text for article in articles]\n",
    "    titles = [title[:title.find('[') - 1] if title.find('[') != -1 else title for title in titles]\n",
    "    #get links\n",
    "    links = [article.find('h3', class_='gs_rt').find('a')['href'] for article in articles]\n",
    "    #get authors\n",
    "    authors = [article.find('div', class_='gs_a').text for article in articles]\n",
    "\n",
    "    # get doi, if it is None, keep link ^ as url of article \n",
    "    dois = [extract_doi_from_webpage(link) for link in links]\n",
    "    dois_links = [doi[0] if doi else None for doi in dois]\n",
    "    final_links = [doi if doi else link for doi, link in zip(dois_links, links)]\n",
    "\n",
    "    \n",
    "    results = pd.concat([results, pd.DataFrame({'titles': titles, 'links': final_links, 'authors': authors, 'dois': dois_links})])\n",
    "\n",
    "results.to_csv('article_info3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    https://www.sciencedirect.com/science/article/...\n",
      "1    https://www.sciencedirect.com/science/article/...\n",
      "2    https://www.sciencedirect.com/science/article/...\n",
      "3    https://www.sciencedirect.com/science/article/...\n",
      "4    https://www.sciencedirect.com/science/article/...\n",
      "                           ...                        \n",
      "5              https://doi.org/10.3390/urbansci2040118\n",
      "6              https://doi.org/10.34306/itsdi.v5i1.609\n",
      "7    http://www.procedia-esem.eu/pdf/issues/2017/no...\n",
      "8    https://doi.org/10.1007/s11356-021-14471-8\",\"P...\n",
      "9    https://www.sciencedirect.com/science/article/...\n",
      "Name: links, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(results['links'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO DOWNLOAD AS PDFS USING DOIS\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "def download_pdfs(dois, file_name):\n",
    "    driver = webdriver.Chrome()  # Initialize web-finder\n",
    "    main_page_url = 'https://sci-hub.ru/'  # Open sci-hub\n",
    "\n",
    "    download_links = []  # List to store download links\n",
    "    \n",
    "    for doi in dois:\n",
    "        try:\n",
    "            driver.get(main_page_url)\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Find search bar -> input DOI\n",
    "            search_bar = driver.find_element(by=By.ID, value='request')\n",
    "            search_bar.send_keys(doi)\n",
    "            search_bar.send_keys(Keys.RETURN)  # To submit the search query\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Find the download button\n",
    "            download_button = driver.find_element(by=By.XPATH, value=\"//button[@onclick]\")\n",
    "            onclick_attribute = download_button.get_attribute('onclick')\n",
    "            pdf_url = onclick_attribute.split(\"'\")[1]\n",
    "            download_link = 'https://sci-hub.ru' + pdf_url\n",
    "            download_links.append(download_link)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while downloading from Sci-Hub: {e}\")\n",
    "    \n",
    "        driver.quit()\n",
    "        return download_links\n",
    "    # Extract URL from the onclick attribute\n",
    "    # Assuming the format is location.href='URL'\n",
    "    pdf_url = onclick_attribute.split(\"'\")[1]\n",
    "    download_link = 'https://sci-hub.ru' + pdf_url\n",
    "    # Download the PDF using requests\n",
    "    response = requests.get(download_link)\n",
    "    with open(f'pdfs/{file_name}.pdf', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF already downloaded\n",
      "An error occurred while downloading from Sci-Hub: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//button[@onclick]\"}\n",
      "  (Session info: chrome=120.0.6099.71); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF772A24D02+56194]\n",
      "\t(No symbol) [0x00007FF7729904B2]\n",
      "\t(No symbol) [0x00007FF7728376AA]\n",
      "\t(No symbol) [0x00007FF7728816D0]\n",
      "\t(No symbol) [0x00007FF7728817EC]\n",
      "\t(No symbol) [0x00007FF7728C4D77]\n",
      "\t(No symbol) [0x00007FF7728A5EBF]\n",
      "\t(No symbol) [0x00007FF7728C2786]\n",
      "\t(No symbol) [0x00007FF7728A5C23]\n",
      "\t(No symbol) [0x00007FF772874A45]\n",
      "\t(No symbol) [0x00007FF772875AD4]\n",
      "\tGetHandleVerifier [0x00007FF772D9D5BB+3695675]\n",
      "\tGetHandleVerifier [0x00007FF772DF6197+4059159]\n",
      "\tGetHandleVerifier [0x00007FF772DEDF63+4025827]\n",
      "\tGetHandleVerifier [0x00007FF772ABF029+687785]\n",
      "\t(No symbol) [0x00007FF77299B508]\n",
      "\t(No symbol) [0x00007FF772997564]\n",
      "\t(No symbol) [0x00007FF7729976E9]\n",
      "\t(No symbol) [0x00007FF772988094]\n",
      "\tBaseThreadInitThunk [0x00007FFDCCF87344+20]\n",
      "\tRtlUserThreadStart [0x00007FFDCDD626B1+33]\n",
      "\n",
      "Error encountered for []\n",
      "An error occurred while downloading from Sci-Hub: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//button[@onclick]\"}\n",
      "  (Session info: chrome=120.0.6099.71); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF772A24D02+56194]\n",
      "\t(No symbol) [0x00007FF7729904B2]\n",
      "\t(No symbol) [0x00007FF7728376AA]\n",
      "\t(No symbol) [0x00007FF7728816D0]\n",
      "\t(No symbol) [0x00007FF7728817EC]\n",
      "\t(No symbol) [0x00007FF7728C4D77]\n",
      "\t(No symbol) [0x00007FF7728A5EBF]\n",
      "\t(No symbol) [0x00007FF7728C2786]\n",
      "\t(No symbol) [0x00007FF7728A5C23]\n",
      "\t(No symbol) [0x00007FF772874A45]\n",
      "\t(No symbol) [0x00007FF772875AD4]\n",
      "\tGetHandleVerifier [0x00007FF772D9D5BB+3695675]\n",
      "\tGetHandleVerifier [0x00007FF772DF6197+4059159]\n",
      "\tGetHandleVerifier [0x00007FF772DEDF63+4025827]\n",
      "\tGetHandleVerifier [0x00007FF772ABF029+687785]\n",
      "\t(No symbol) [0x00007FF77299B508]\n",
      "\t(No symbol) [0x00007FF772997564]\n",
      "\t(No symbol) [0x00007FF7729976E9]\n",
      "\t(No symbol) [0x00007FF772988094]\n",
      "\tBaseThreadInitThunk [0x00007FFDCCF87344+20]\n",
      "\tRtlUserThreadStart [0x00007FFDCDD626B1+33]\n",
      "\n",
      "Error encountered for []\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "An error occurred while downloading from Sci-Hub: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//button[@onclick]\"}\n",
      "  (Session info: chrome=120.0.6099.71); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF772A24D02+56194]\n",
      "\t(No symbol) [0x00007FF7729904B2]\n",
      "\t(No symbol) [0x00007FF7728376AA]\n",
      "\t(No symbol) [0x00007FF7728816D0]\n",
      "\t(No symbol) [0x00007FF7728817EC]\n",
      "\t(No symbol) [0x00007FF7728C4D77]\n",
      "\t(No symbol) [0x00007FF7728A5EBF]\n",
      "\t(No symbol) [0x00007FF7728C2786]\n",
      "\t(No symbol) [0x00007FF7728A5C23]\n",
      "\t(No symbol) [0x00007FF772874A45]\n",
      "\t(No symbol) [0x00007FF772875AD4]\n",
      "\tGetHandleVerifier [0x00007FF772D9D5BB+3695675]\n",
      "\tGetHandleVerifier [0x00007FF772DF6197+4059159]\n",
      "\tGetHandleVerifier [0x00007FF772DEDF63+4025827]\n",
      "\tGetHandleVerifier [0x00007FF772ABF029+687785]\n",
      "\t(No symbol) [0x00007FF77299B508]\n",
      "\t(No symbol) [0x00007FF772997564]\n",
      "\t(No symbol) [0x00007FF7729976E9]\n",
      "\t(No symbol) [0x00007FF772988094]\n",
      "\tBaseThreadInitThunk [0x00007FFDCCF87344+20]\n",
      "\tRtlUserThreadStart [0x00007FFDCDD626B1+33]\n",
      "\n",
      "Error encountered for []\n",
      "An error occurred while downloading from Sci-Hub: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//button[@onclick]\"}\n",
      "  (Session info: chrome=120.0.6099.71); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF772A24D02+56194]\n",
      "\t(No symbol) [0x00007FF7729904B2]\n",
      "\t(No symbol) [0x00007FF7728376AA]\n",
      "\t(No symbol) [0x00007FF7728816D0]\n",
      "\t(No symbol) [0x00007FF7728817EC]\n",
      "\t(No symbol) [0x00007FF7728C4D77]\n",
      "\t(No symbol) [0x00007FF7728A5EBF]\n",
      "\t(No symbol) [0x00007FF7728C2786]\n",
      "\t(No symbol) [0x00007FF7728A5C23]\n",
      "\t(No symbol) [0x00007FF772874A45]\n",
      "\t(No symbol) [0x00007FF772875AD4]\n",
      "\tGetHandleVerifier [0x00007FF772D9D5BB+3695675]\n",
      "\tGetHandleVerifier [0x00007FF772DF6197+4059159]\n",
      "\tGetHandleVerifier [0x00007FF772DEDF63+4025827]\n",
      "\tGetHandleVerifier [0x00007FF772ABF029+687785]\n",
      "\t(No symbol) [0x00007FF77299B508]\n",
      "\t(No symbol) [0x00007FF772997564]\n",
      "\t(No symbol) [0x00007FF7729976E9]\n",
      "\t(No symbol) [0x00007FF772988094]\n",
      "\tBaseThreadInitThunk [0x00007FFDCCF87344+20]\n",
      "\tRtlUserThreadStart [0x00007FFDCDD626B1+33]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Service.__del__ at 0x0000020E61D523B0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Gur Levy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\selenium\\webdriver\\common\\service.py\", line 185, in __del__\n",
      "    self.stop()\n",
      "  File \"C:\\Users\\Gur Levy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\selenium\\webdriver\\common\\service.py\", line 146, in stop\n",
      "    self.send_remote_shutdown_command()\n",
      "  File \"C:\\Users\\Gur Levy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\selenium\\webdriver\\common\\service.py\", line 126, in send_remote_shutdown_command\n",
      "    request.urlopen(f\"{self.service_url}/shutdown\")\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\urllib\\request.py\", line 216, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\urllib\\request.py\", line 519, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\urllib\\request.py\", line 536, in _open\n",
      "    result = self._call_chain(self.handle_open, protocol, protocol +\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\urllib\\request.py\", line 496, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\urllib\\request.py\", line 1377, in http_open\n",
      "    return self.do_open(http.client.HTTPConnection, req)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\urllib\\request.py\", line 1348, in do_open\n",
      "    h.request(req.get_method(), req.selector, req.data, headers,\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py\", line 1283, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py\", line 1329, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py\", line 1278, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py\", line 1038, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py\", line 976, in send\n",
      "    self.connect()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py\", line 942, in connect\n",
      "    self.sock = self._create_connection(\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\socket.py\", line 833, in create_connection\n",
      "    sock.connect(sa)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error encountered for []\n",
      "An error occurred while downloading from Sci-Hub: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=120.0.6099.71)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF772A24D02+56194]\n",
      "\t(No symbol) [0x00007FF7729904B2]\n",
      "\t(No symbol) [0x00007FF7728376AA]\n",
      "\t(No symbol) [0x00007FF772810AFD]\n",
      "\t(No symbol) [0x00007FF7728AC9AB]\n",
      "\t(No symbol) [0x00007FF7728C201F]\n",
      "\t(No symbol) [0x00007FF7728A5C23]\n",
      "\t(No symbol) [0x00007FF772874A45]\n",
      "\t(No symbol) [0x00007FF772875AD4]\n",
      "\tGetHandleVerifier [0x00007FF772D9D5BB+3695675]\n",
      "\tGetHandleVerifier [0x00007FF772DF6197+4059159]\n",
      "\tGetHandleVerifier [0x00007FF772DEDF63+4025827]\n",
      "\tGetHandleVerifier [0x00007FF772ABF029+687785]\n",
      "\t(No symbol) [0x00007FF77299B508]\n",
      "\t(No symbol) [0x00007FF772997564]\n",
      "\t(No symbol) [0x00007FF7729976E9]\n",
      "\t(No symbol) [0x00007FF772988094]\n",
      "\tBaseThreadInitThunk [0x00007FFDCCF87344+20]\n",
      "\tRtlUserThreadStart [0x00007FFDCDD626B1+33]\n",
      "\n",
      "Error encountered for []\n",
      "PDF already downloaded\n",
      "PDF already downloaded\n",
      "An error occurred while downloading from Sci-Hub: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=120.0.6099.71)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF772A24D02+56194]\n",
      "\t(No symbol) [0x00007FF7729904B2]\n",
      "\t(No symbol) [0x00007FF7728376AA]\n",
      "\t(No symbol) [0x00007FF772810AFD]\n",
      "\t(No symbol) [0x00007FF7728AC9AB]\n",
      "\t(No symbol) [0x00007FF7728C201F]\n",
      "\t(No symbol) [0x00007FF7728A5C23]\n",
      "\t(No symbol) [0x00007FF772874A45]\n",
      "\t(No symbol) [0x00007FF772875AD4]\n",
      "\tGetHandleVerifier [0x00007FF772D9D5BB+3695675]\n",
      "\tGetHandleVerifier [0x00007FF772DF6197+4059159]\n",
      "\tGetHandleVerifier [0x00007FF772DEDF63+4025827]\n",
      "\tGetHandleVerifier [0x00007FF772ABF029+687785]\n",
      "\t(No symbol) [0x00007FF77299B508]\n",
      "\t(No symbol) [0x00007FF772997564]\n",
      "\t(No symbol) [0x00007FF7729976E9]\n",
      "\t(No symbol) [0x00007FF772988094]\n",
      "\tBaseThreadInitThunk [0x00007FFDCCF87344+20]\n",
      "\tRtlUserThreadStart [0x00007FFDCDD626B1+33]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LOOP THROUGH URLS\n",
    "import os\n",
    "data = pd.read_csv('article_info3.csv')\n",
    "list_error_links = []  # Define an empty list to store error links if encountered\n",
    "\n",
    "# Function to check if a file exists at a given path\n",
    "def file_exists(file_path):\n",
    "    return os.path.exists(file_path)\n",
    "\n",
    "# Loop through each row in the DataFrame 'data'\n",
    "for index, row in data.iterrows():\n",
    "    pdf_file_path = f'pdfs/{index}.pdf'  # Define the path for the PDF file\n",
    "    if file_exists(pdf_file_path):\n",
    "        print(\"PDF already downloaded\")\n",
    "    else:\n",
    "        if row['links'] in list_error_links:\n",
    "            print('Error already encountered')\n",
    "        else:\n",
    "            url_returned = download_pdfs(row['links'], file_name='pdfs')  \n",
    "            if url_returned is not None:\n",
    "                list_error_links.append(url_returned)\n",
    "                print(f'Error encountered for {url_returned}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
